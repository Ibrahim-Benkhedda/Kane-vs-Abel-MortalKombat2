import retro
import cv2
import random 
import os 
import numpy as np

from mk_ai.utils import ActionGenerator
from gymnasium.spaces import Discrete, Box 

class MkEnvWrapper(retro.RetroEnv):
    """
    MkEnvWrapper is a custom retro environment wrapper class for the Mortal Kombat II: Genesis using the Retro library.
    It extends the RetroEnv class and provides additional functionality for action mapping and custom reward calculation.

    Attributes:
        action_generator (ActionGenerator): An instance of the ActionGenerator class to handle action mappings.
        _action_mapping (list): A list of action mappings generated by the ActionGenerator.
        action_space (Discrete): The discrete action space for the agent.
        max_health (int): The maximum health value for the player and enemy.
        player_health_prev (int): The previous health value of the player.
        enemy_health_prev (int): The previous health value of the enemy.
        states (list): A list of states to be used for training the agent to alternate between states.

    Methods:

        __init__: Initializes the MkEnvWrapper with the specified game, health, reward, and penalty values.
        reset: Resets the environment and initializes health tracking.
        step: Takes the specified action, computes the custom reward, and returns the observation, total reward, done flag,
                truncated flag, and info.
        _preprocess_frame: Preprocesses the frame before passing it to the model.
        _compute_reward: Computes the custom reward based on the health and round status of the player and enemy.
        set_states: Set the states list to the new states list.
    """

    def __init__(
        self,
        game: str,
        max_health: int = 120,
        record: str = None,
        state: str = None,
        states: list[str] = None,
        config_path: str = None,
        **kwargs
    ):
        super().__init__(game, record=record, **kwargs)
        
        self._setup_action_space(config_path=config_path)

        # obs space adjustment
        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)

        # if one state is provided, use it
        self.state = state
        
        # if a list of states is provided, use it
        # stores a list of states to be used for training the agent to alternate between states
        self.states = states

        # env info tracking for reward computation
        self.max_health = max_health
        self.player_health_prev = None    
        self.enemy_health_prev = None
        self.prev_player_round_won = 0
        self.prev_enemy_round_won = 0

    def reset(self, **kwargs) -> tuple[np.ndarray, dict]:
        """
        Resets the environment to an initial state and initializes health tracking.

        Parameters:
            **kwargs: Additional arguments to pass to the parent class's reset method.

        Returns:
            tuple: A tuple containing the initial observation and additional info.
        """
        
        if self.states is not None:
            # if states list is provided, randomly select a state from the list
            state = random.choice(self.states)
        else:
            # if no states list is provided, use the current state
            state = self.state
        
        print(f"[MkEnvWrapper] Loading state: {state}")
        
        self.load_state(state)

        # reset the environment with the selected state
        obs, info = super().reset(**kwargs)

        # Initialize health tracking
        self.player_health_prev = info.get("health", self.max_health)
        self.enemy_health_prev = info.get("enemy_health", self.max_health)
        
        # print("Player Health:", self.player_health_prev)
        # print("Enemy Health:", self.enemy_health_prev)

        self.prev_player_round_won = 0
        self.prev_enemy_round_won = 0

        # preprocess the returned observation
        obs = self._preprocess_frame(obs)

        # print("Reset: ", obs.shape)

        return obs, info

    def step(self, action: list[str]) -> tuple[np.ndarray, float, bool, bool, dict]:
        """
        Execute a step in the environment using the given action.

        Parameters:
            action (int): The action to be taken, which is mapped to a predefined action.

        Returns:
            tuple: A tuple containing:
                - obs (object): The observation after taking the action.
                - total_reward (float): The combined reward from the environment and custom reward.
                - done (bool): Whether the episode has ended.
                - truncated (bool): Whether the episode was truncated.
                - info (dict): Additional information from the environment, including health and round status.
        """
        # Take the action using the predefined action mapping 
        obs, retro_reward, done, truncated, info = super().step(self._action_mapping[action])

        # Retrieve RAM for health and round status
        player_health = info.get("health", self.player_health_prev)
        enemy_health = info.get("enemy_health", self.enemy_health_prev)
        rounds_won = info.get("rounds_won", 0)
        enemy_rounds_won = info.get("enemy_rounds_won", 0)

        # Compute custom reward
        reward = self._compute_reward(player_health, enemy_health, rounds_won, enemy_rounds_won)
        
        # Update previous health
        self.player_health_prev = player_health
        self.enemy_health_prev = enemy_health
        self.prev_player_round_won = rounds_won
        self.prev_enemy_round_won = enemy_rounds_won

        # print(rounds_won, enemy_rounds_won, reward)
        # print(player_x_position, enemy_x_position)
        # end episode (in MK2 a game ends when a player wins 2 rounds)
        if rounds_won == 2 or enemy_rounds_won == 2:
            # print("Episode Ended")
            done = True

        # preprocess the NEXT observation
        obs = self._preprocess_frame(obs)

        # print("Step: ", obs.shape)
        return obs, reward, done, truncated, info

    def _preprocess_frame(self, obs: np.ndarray) -> np.ndarray:
        
        """
        Preprocess the frame before passing it to the model.

        converts the observation to grayscale, resizes it to 84x84, 
        and converts it to uint8 format.

        Parameters:
            obs (object): The raw observation from the environment.

        Returns:
            object: The preprocessed observation.
        """
        # convert the observation to grayscale
        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
        # resize the observation to 84x84
        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
        # converts the resized observation to uint8
        processed_obs = np.expand_dims(resized.astype(np.uint8), axis=-1)
        return processed_obs


    def _compute_reward(self, player_health: float, enemy_health: float, rounds_won: int, enemy_rounds_won: int) -> float:
        
        """
        Compute the custom reward based on the health and round status of the player and enemy.

        Parameters:
            player_health (float): The current health of the player.
            enemy_health (float): The current health of the enemy.
            rounds_won (int): The number of rounds won by the player.
            enemy_rounds_won (int): The number of rounds won by the enemy.

        Returns:
            float: The computed reward value.
        """
        reward = 0

        # Reward for dealing damage
        reward += 1.0 * max(0, self.enemy_health_prev - enemy_health)
        # Penalty for taking damage
        reward -= 1.0 * max(0, self.player_health_prev - player_health)

        # Bonus for winning a round
        if rounds_won > self.prev_player_round_won:
            reward += 1.0 * self.max_health**((player_health + 1) / (self.max_health + 1))

        # Penalty for losing a round
        if enemy_rounds_won > self.prev_enemy_round_won:
            reward -= 1.0 * self.max_health**((enemy_health + 1) / (self.max_health + 1))
        # print(rounds_won, enemy_rounds_won, self.prev_player_round_won, self.prev_enemy_round_won)
        return reward 
    
    def set_states(self, new_states: list[str]) -> None:
        """
        Set the states list to the new states list.

        Parameters:
            new_states (list): A list of new states to be added to the states list.
        """
        self.states = new_states

    def _setup_action_space(self, config_path: str = None) -> None:
        """
        Internal helper method to load the YAML config 
        and initialize the ActionGenerator with the resulting data.
        """
        # If user didn't supply a config path, build a full path relative to THIS file
        if config_path is None:
            current_dir = os.path.dirname(__file__)  # directory containing mk_env.py
            config_path = os.path.join(current_dir, "..", "configs", "env_config.yaml")
            # the env_config.yaml is in mk_ai/configs relative to mk_ai/wrappers
        # 1. initialize the ActionGenerator class and add predefined buttons
        self.action_generator = ActionGenerator(filename=config_path)

        # 2. Build the internal data structures
        self.action_generator.build() 

        # 3. generate action mappings from the ActionGenerator class
        self._action_mapping = self.action_generator.binary_mapping

        # 4. convert multiple button combinations into a single discrete action
        # that the agent can select at each step.
        self.action_space = Discrete(len(self._action_mapping))


if __name__ == "__main__":
    # 1) Create the environment
    env = MkEnvWrapper(game="MortalKombatII-Genesis", state="Level1.LiuKangVsJax")

    # 2) Reset it once to see if everything loads and prints the info
    obs, info = env.reset()

    # Print out a small summary
    print(f"[MAIN] Observation shape: {obs.shape}")
    print(f"[MAIN] Initial info dict: {info}")
    print(f"[MAIN] Action space size: {env.action_space}")