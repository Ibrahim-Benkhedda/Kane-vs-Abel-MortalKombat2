# DQN Training Loop Steps

## Initialize Environment and Replay Buffer:

The agent interacts with the environment to gather experience tuples 
(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²,ğ‘‘) (s,a,r,sâ€²,d), where ğ‘‘ is a done flag indicating the end of an episode. These are stored in a replay buffer.

## Initialize Q-Networks:

- A policy network (for Q-value estimation).
- A target network (a copy of the policy network, updated periodically for stability).

## Iterate Over Episodes:

### For each episode:
1. **Reset Environment**: Start with an initial state ğ‘ .
2. **Choose Action**: Use an epsilon-greedy policy to balance exploration (random actions) and exploitation (actions with the highest predicted Q-value).
3. **Step in Environment**: Execute action ğ‘, observe next state ğ‘ â€², reward ğ‘Ÿ, and done flag ğ‘‘.
4. **Store in Replay Buffer**: Add (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²,ğ‘‘) (s,a,r,sâ€²,d) to the buffer.
5. **Sample Mini-Batch**: Randomly sample experiences from the replay buffer for training.
6. **Compute Targets**: Use the target network to compute the target Q-value:  
   $ ğ‘¦ = ğ‘Ÿ + ğ›¾ \max_{ğ‘} Q_{\text{target}}(ğ‘ â€²,ğ‘) $  
   for non-terminal states.
7. **Update Q-Network**: Minimize the loss using gradient descent.:  
   $ ğ¿ = \frac{1}{ğ‘} \sum (ğ‘¦ âˆ’ Q_{\text{policy}}(ğ‘ ,ğ‘))^2 $  
8. **Update Target Network**: Periodically update the target network to the current policy network.
