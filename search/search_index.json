{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Kane vs Abel: Mortal Kombat II Arena Welcome to the Kane vs Abel documentation! Click here to continue to the Arena documentation // Auto-redirect to arena.md window.location.href = 'arena.md';","title":"Home"},{"location":"index.html#kane-vs-abel-mortal-kombat-ii-arena","text":"Welcome to the Kane vs Abel documentation! Click here to continue to the Arena documentation // Auto-redirect to arena.md window.location.href = 'arena.md';","title":"Kane vs Abel: Mortal Kombat II Arena"},{"location":"action_space.html","text":"Action Space in Kane vs Abel Mortal Kombat II This document explains how the action system works in the Mortal Kombat II environment, including action generation, mapping, and how different agents interface with it. Action Space Representations The action system uses three core representations that are translated between different components: Button Names : Human-readable commands ( LEFT , RIGHT , A , B , etc.) Binary Arrays : Stable-Retro's internal format ( [0,0,1,0,0,0,0,0] ) Action IDs : Integer indices used to reference specific actions ( 0 , 1 , 2 , etc.) Action Generator The ActionGenerator class creates and manages mappings between these representations: from mk_ai.utils import ActionGenerator # Load from YAML configuration action_gen = ActionGenerator(filename=\"env_config.yaml\") action_gen.build() # Must be called explicitly to create mappings # Access the generated mappings action_map = action_gen.action_map # Maps names to IDs: {\"LEFT\": 1, \"RIGHT\": 2} YAML Configuration The env_config.yaml file defines the buttons and valid action combinations: buttons: - B - A - MODE - START - UP - DOWN - LEFT - RIGHT - C - Y - X - Z actions: - [] # Neutral - [LEFT] - [RIGHT] - [LEFT, DOWN] - [RIGHT, DOWN] # ...more combinations Generated Mappings The ActionGenerator creates several key mappings: # Button list - individual inputs buttons = ['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z'] # Action combinations with auto-generated names action_map = { 'NEUTRAL': 0, 'LEFT': 1, 'RIGHT': 2, 'LEFT_DOWN': 3, 'RIGHT_DOWN': 4, # ... } # Binary representation for each action binary_mapping = { 0: [0,0,0,0,0,0,0,0,0,0,0,0], # NEUTRAL 1: [0,0,0,0,0,0,1,0,0,0,0,0], # LEFT 2: [0,0,0,0,0,0,0,1,0,0,0,0], # RIGHT # ... } Agent Action Processing RL Agents (DQN, Double DQN, Dueling DQN) RL agents directly output action IDs: # Agent internally selects action from discrete space action_id = agent.select_action(observation, info) # Returns integer ID # MkEnvWrapper handles conversion to binary format for Stable-Retro binary_action = env_wrapper.action_mapping[action_id] # e.g., [0,0,1,0,0,0,0,0] Behavior Tree Agents BT agents process actions through multiple steps: YAML Definition : Actions are defined with string names in the BT file: ```yaml type: Action name: \"Jump Attack\" properties: action_id: UP_A # String name matching action_map keys frames_needed: 5 ``` Conversion in BTLoader : String names are converted to action IDs during BT construction: python # In BTLoader.gen_node(): action_id_name = properties.get(\"action_id\") # \"UP_A\" from YAML action_id = self.action_map.get(action_id_name, 0) # Maps to integer ID Action Selection : The BT returns the action ID of the active node: python # In BTAgent.tick(): status = self.bt_root.tick(self.context) action_id = self.bt_root.get_current_action() return action_id # Returns integer for environment Binary Conversion : The environment wrapper converts the ID to binary format Human Agents Human agents convert keyboard inputs to action IDs: # 1. Key pressed \u2192 button name via key_map button_name = self.key_map.get(key) # e.g., LEFT, RIGHT # 2. Build binary array from pressed buttons binary_array = [0] * len(self.buttons) for button in pressed_buttons: if button in self.buttons: idx = self.buttons.index(button) binary_array[idx] = 1 # 3. Find matching action ID from the binary array for action_id, action_binary in enumerate(self.action_mapping): if action_binary == binary_array: return action_id Complete Action Flow \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Agent Selection \u2551 \u2551 Environment Input \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 \u2551 \u2551 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2551 \u2502 Human: \u2502 \u2551 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Keyboard Input \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2510 \u2551 \u2502 Stable-Retro \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2551 \u2502 Environment \u2502 \u2551 \u2551 \u2551 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 RL: \u2502 \u2551 \u2514\u2500>\u2502 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Neural Network \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2500>\u2502 Action \u2502\u2500\u2500>\u2551\u2500\u2500>\u2502 \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u250c\u2500>\u2502 ID \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2551 \u2502 \u2502 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 BT: \u2502 \u2551 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Decision Tree \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2551 \u2551 \u2551 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u25b2 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Action Conversion \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 ID <=> Binary \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Visualizing the Flow +------------------+ +---------------------------+ +-----------------------+ | Agent's Action | ---> | Action Mapping | ---> | Game Environment | | (Discrete Index) | | (Binary Array) | | (Button Presses) | +------------------+ +---------------------------+ +-----------------------+ | 0 | | [0, 0, 0, 0, 0, 0, 0, 0] | | NEUTRAL | | 1 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'LEFT' | | 2 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'RIGHT' | | 3 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'LEFT' + 'DOWN' | | 4 | | [0, 0, 0, 0, 0, 0, 1, 0] | | 'RIGHT' + 'DOWN' | +------------------+ +---------------------------+ +-----------------------+ Implementation Details Shared Action Space All agents share the same action space defined in env_config.yaml. This ensures: Consistent behavior across agent types Fair comparisons between different agents Compatibility with the Stable-Retro environment Adding Custom Actions To add a new action: Add the button combination to env_config.yaml: ```yaml actions: # Existing actions... [DOWN] # Add standalone crouch action ``` Rebuild the action mappings and update affected agents. Debugging Tips Check the action_map to see available actions: print(action_gen.action_map) Ensure BT YAML files use action names that match the keys in action_map If actions aren't working, verify they exist in env_config.yaml Use print(binary_mapping[action_id]) to see the binary representation Related Files mk_ai/utils/action_generator.py : Contains ActionGenerator class mk_ai/configs/env_config.yaml : Defines buttons and valid actions mk_ai/agents/bt_agent.py : BT agent implementation mk_ai/agents/BT/loader.py : Loads and processes BT actions","title":"Action Space"},{"location":"action_space.html#action-space-in-kane-vs-abel-mortal-kombat-ii","text":"This document explains how the action system works in the Mortal Kombat II environment, including action generation, mapping, and how different agents interface with it.","title":"Action Space in Kane vs Abel Mortal Kombat II"},{"location":"action_space.html#action-space-representations","text":"The action system uses three core representations that are translated between different components: Button Names : Human-readable commands ( LEFT , RIGHT , A , B , etc.) Binary Arrays : Stable-Retro's internal format ( [0,0,1,0,0,0,0,0] ) Action IDs : Integer indices used to reference specific actions ( 0 , 1 , 2 , etc.)","title":"Action Space Representations"},{"location":"action_space.html#action-generator","text":"The ActionGenerator class creates and manages mappings between these representations: from mk_ai.utils import ActionGenerator # Load from YAML configuration action_gen = ActionGenerator(filename=\"env_config.yaml\") action_gen.build() # Must be called explicitly to create mappings # Access the generated mappings action_map = action_gen.action_map # Maps names to IDs: {\"LEFT\": 1, \"RIGHT\": 2}","title":"Action Generator"},{"location":"action_space.html#yaml-configuration","text":"The env_config.yaml file defines the buttons and valid action combinations: buttons: - B - A - MODE - START - UP - DOWN - LEFT - RIGHT - C - Y - X - Z actions: - [] # Neutral - [LEFT] - [RIGHT] - [LEFT, DOWN] - [RIGHT, DOWN] # ...more combinations","title":"YAML Configuration"},{"location":"action_space.html#generated-mappings","text":"The ActionGenerator creates several key mappings: # Button list - individual inputs buttons = ['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z'] # Action combinations with auto-generated names action_map = { 'NEUTRAL': 0, 'LEFT': 1, 'RIGHT': 2, 'LEFT_DOWN': 3, 'RIGHT_DOWN': 4, # ... } # Binary representation for each action binary_mapping = { 0: [0,0,0,0,0,0,0,0,0,0,0,0], # NEUTRAL 1: [0,0,0,0,0,0,1,0,0,0,0,0], # LEFT 2: [0,0,0,0,0,0,0,1,0,0,0,0], # RIGHT # ... }","title":"Generated Mappings"},{"location":"action_space.html#agent-action-processing","text":"","title":"Agent Action Processing"},{"location":"action_space.html#rl-agents-dqn-double-dqn-dueling-dqn","text":"RL agents directly output action IDs: # Agent internally selects action from discrete space action_id = agent.select_action(observation, info) # Returns integer ID # MkEnvWrapper handles conversion to binary format for Stable-Retro binary_action = env_wrapper.action_mapping[action_id] # e.g., [0,0,1,0,0,0,0,0]","title":"RL Agents (DQN, Double DQN, Dueling DQN)"},{"location":"action_space.html#behavior-tree-agents","text":"BT agents process actions through multiple steps: YAML Definition : Actions are defined with string names in the BT file: ```yaml type: Action name: \"Jump Attack\" properties: action_id: UP_A # String name matching action_map keys frames_needed: 5 ``` Conversion in BTLoader : String names are converted to action IDs during BT construction: python # In BTLoader.gen_node(): action_id_name = properties.get(\"action_id\") # \"UP_A\" from YAML action_id = self.action_map.get(action_id_name, 0) # Maps to integer ID Action Selection : The BT returns the action ID of the active node: python # In BTAgent.tick(): status = self.bt_root.tick(self.context) action_id = self.bt_root.get_current_action() return action_id # Returns integer for environment Binary Conversion : The environment wrapper converts the ID to binary format","title":"Behavior Tree Agents"},{"location":"action_space.html#human-agents","text":"Human agents convert keyboard inputs to action IDs: # 1. Key pressed \u2192 button name via key_map button_name = self.key_map.get(key) # e.g., LEFT, RIGHT # 2. Build binary array from pressed buttons binary_array = [0] * len(self.buttons) for button in pressed_buttons: if button in self.buttons: idx = self.buttons.index(button) binary_array[idx] = 1 # 3. Find matching action ID from the binary array for action_id, action_binary in enumerate(self.action_mapping): if action_binary == binary_array: return action_id","title":"Human Agents"},{"location":"action_space.html#complete-action-flow","text":"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Agent Selection \u2551 \u2551 Environment Input \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 \u2551 \u2551 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2551 \u2502 Human: \u2502 \u2551 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Keyboard Input \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2510 \u2551 \u2502 Stable-Retro \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2551 \u2502 Environment \u2502 \u2551 \u2551 \u2551 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 RL: \u2502 \u2551 \u2514\u2500>\u2502 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Neural Network \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2500>\u2502 Action \u2502\u2500\u2500>\u2551\u2500\u2500>\u2502 \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u250c\u2500>\u2502 ID \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2551 \u2502 \u2502 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2551 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 BT: \u2502 \u2551 \u2502 \u2551 \u2502 \u2502 \u2551 \u2551 \u2502 Decision Tree \u2502\u2500\u2500>\u2551\u2500\u2500\u2500\u2518 \u2551 \u2502 \u2502 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2551 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2551 \u2551 \u2551 \u2551 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u25b2 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Action Conversion \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 ID <=> Binary \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Complete Action Flow"},{"location":"action_space.html#visualizing-the-flow","text":"+------------------+ +---------------------------+ +-----------------------+ | Agent's Action | ---> | Action Mapping | ---> | Game Environment | | (Discrete Index) | | (Binary Array) | | (Button Presses) | +------------------+ +---------------------------+ +-----------------------+ | 0 | | [0, 0, 0, 0, 0, 0, 0, 0] | | NEUTRAL | | 1 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'LEFT' | | 2 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'RIGHT' | | 3 | | [0, 0, 0, 0, 0, 0, 0, 0] | | 'LEFT' + 'DOWN' | | 4 | | [0, 0, 0, 0, 0, 0, 1, 0] | | 'RIGHT' + 'DOWN' | +------------------+ +---------------------------+ +-----------------------+","title":"Visualizing the Flow"},{"location":"action_space.html#implementation-details","text":"","title":"Implementation Details"},{"location":"action_space.html#shared-action-space","text":"All agents share the same action space defined in env_config.yaml. This ensures: Consistent behavior across agent types Fair comparisons between different agents Compatibility with the Stable-Retro environment","title":"Shared Action Space"},{"location":"action_space.html#adding-custom-actions","text":"To add a new action: Add the button combination to env_config.yaml: ```yaml actions: # Existing actions... [DOWN] # Add standalone crouch action ``` Rebuild the action mappings and update affected agents.","title":"Adding Custom Actions"},{"location":"action_space.html#debugging-tips","text":"Check the action_map to see available actions: print(action_gen.action_map) Ensure BT YAML files use action names that match the keys in action_map If actions aren't working, verify they exist in env_config.yaml Use print(binary_mapping[action_id]) to see the binary representation","title":"Debugging Tips"},{"location":"action_space.html#related-files","text":"mk_ai/utils/action_generator.py : Contains ActionGenerator class mk_ai/configs/env_config.yaml : Defines buttons and valid actions mk_ai/agents/bt_agent.py : BT agent implementation mk_ai/agents/BT/loader.py : Loads and processes BT actions","title":"Related Files"},{"location":"adding_diff_game.html","text":"Adding New Games to the Kane vs Abel Arena Framework This guide outlines the workflow for integrating new Stable-Retro compatible games into the Arena framework. Overview The Arena framework was designed for Mortal Kombat II but can be extended to other 2-player games. This document outlines the key steps and files to modify for successful integration. Integration Workflow 1. Import Game ROM into Stable-Retro # Import your ROM python -m retro.import /path/to/game.md Verify import: # Verify game appears in list python -c \"import retro; print([g for g in retro.data.list_games() if 'YourGame' in g])\" 2. Create Game States Launch the game with Stable-Retro UI: python -m retro.ui.play --game YourGame-SystemName Press F9 at meaningful starting points (character select, match start) States are saved in ~/.local/share/stable-retro/data/YourGame-SystemName/states/ 3. Define Game Memory Mapping Create or modify: - ~/.local/share/stable-retro/data/YourGame-SystemName/data.json Key variables to map: - Player health - Enemy health - Player position - Enemy position - Round counters - Any game-specific state information 4. Configure Button Layout Create a new config: - src/mk_ai/configs/your_game_config.yaml Define: - Available buttons - Valid button combinations/actions 5. Create Game-Specific Environment Wrapper Modify or create: - src/mk_ai/wrappers/your_game_env.py Implement: - Game-specific observation processing - Custom reward functions - State tracking relevant to your game 6. Extend MultiAgent Support Modify or create: - src/mk_ai/wrappers/multiagent_your_game.py Implement: - Proper handling of two-player actions - Player-specific observation processing - Game-specific multiagent interactions 7. Update Arena Configuration Modify: - arena_config.py Add: - Support for your game in the config class - Game state selection options 8. Update Command Line Interface Modify: - arena.py Add: - Game selection parameters - State selection parameters - Game-specific options Testing Your Integration Start with basic functionality: python arena.py --game YourGame-SystemName --state YourState Test human controls: python arena.py --game YourGame-SystemName --p1-type human --p2-type human Test with AI agents: python arena.py --game YourGame-SystemName --p1-type bt --p1-bt-file your_bt.yaml Troubleshooting Tips Memory addresses incorrect : Use RAM watch in Stable-Retro UI to find correct addresses Game not launching : Check ROM import and state file existence Actions not working : Verify button configuration and action mapping Agent not responding : Ensure observation processing works properly Key Files Reference File Purpose What to Modify env_config.yaml Button/action definitions Add game-specific actions mk_env.py Base environment wrapper Extend for game-specific behavior multiagent_mk_env.py Multiagent support Extend for 2-player support arena_config.py Framework configuration Add game to available options arena.py Main application Add game-specific CLI options Example Workflow For a new fighting game: Import ROM and create starting state at match beginning Map health, position and round variables in data.json Define basic moves in a new config YAML file Create environment wrapper handling game-specific rewards Extend multiagent support for 2-player mode Update arena configuration and CLI Test with human controls first, then with agents By following this workflow, you can integrate most Stable-Retro compatible games into the Arena framework for AI research and evaluation.","title":"Additional Games"},{"location":"adding_diff_game.html#adding-new-games-to-the-kane-vs-abel-arena-framework","text":"This guide outlines the workflow for integrating new Stable-Retro compatible games into the Arena framework.","title":"Adding New Games to the Kane vs Abel Arena Framework"},{"location":"adding_diff_game.html#overview","text":"The Arena framework was designed for Mortal Kombat II but can be extended to other 2-player games. This document outlines the key steps and files to modify for successful integration.","title":"Overview"},{"location":"adding_diff_game.html#integration-workflow","text":"","title":"Integration Workflow"},{"location":"adding_diff_game.html#1-import-game-rom-into-stable-retro","text":"# Import your ROM python -m retro.import /path/to/game.md Verify import: # Verify game appears in list python -c \"import retro; print([g for g in retro.data.list_games() if 'YourGame' in g])\"","title":"1. Import Game ROM into Stable-Retro"},{"location":"adding_diff_game.html#2-create-game-states","text":"Launch the game with Stable-Retro UI: python -m retro.ui.play --game YourGame-SystemName Press F9 at meaningful starting points (character select, match start) States are saved in ~/.local/share/stable-retro/data/YourGame-SystemName/states/","title":"2. Create Game States"},{"location":"adding_diff_game.html#3-define-game-memory-mapping","text":"Create or modify: - ~/.local/share/stable-retro/data/YourGame-SystemName/data.json Key variables to map: - Player health - Enemy health - Player position - Enemy position - Round counters - Any game-specific state information","title":"3. Define Game Memory Mapping"},{"location":"adding_diff_game.html#4-configure-button-layout","text":"Create a new config: - src/mk_ai/configs/your_game_config.yaml Define: - Available buttons - Valid button combinations/actions","title":"4. Configure Button Layout"},{"location":"adding_diff_game.html#5-create-game-specific-environment-wrapper","text":"Modify or create: - src/mk_ai/wrappers/your_game_env.py Implement: - Game-specific observation processing - Custom reward functions - State tracking relevant to your game","title":"5. Create Game-Specific Environment Wrapper"},{"location":"adding_diff_game.html#6-extend-multiagent-support","text":"Modify or create: - src/mk_ai/wrappers/multiagent_your_game.py Implement: - Proper handling of two-player actions - Player-specific observation processing - Game-specific multiagent interactions","title":"6. Extend MultiAgent Support"},{"location":"adding_diff_game.html#7-update-arena-configuration","text":"Modify: - arena_config.py Add: - Support for your game in the config class - Game state selection options","title":"7. Update Arena Configuration"},{"location":"adding_diff_game.html#8-update-command-line-interface","text":"Modify: - arena.py Add: - Game selection parameters - State selection parameters - Game-specific options","title":"8. Update Command Line Interface"},{"location":"adding_diff_game.html#testing-your-integration","text":"Start with basic functionality: python arena.py --game YourGame-SystemName --state YourState Test human controls: python arena.py --game YourGame-SystemName --p1-type human --p2-type human Test with AI agents: python arena.py --game YourGame-SystemName --p1-type bt --p1-bt-file your_bt.yaml","title":"Testing Your Integration"},{"location":"adding_diff_game.html#troubleshooting-tips","text":"Memory addresses incorrect : Use RAM watch in Stable-Retro UI to find correct addresses Game not launching : Check ROM import and state file existence Actions not working : Verify button configuration and action mapping Agent not responding : Ensure observation processing works properly","title":"Troubleshooting Tips"},{"location":"adding_diff_game.html#key-files-reference","text":"File Purpose What to Modify env_config.yaml Button/action definitions Add game-specific actions mk_env.py Base environment wrapper Extend for game-specific behavior multiagent_mk_env.py Multiagent support Extend for 2-player support arena_config.py Framework configuration Add game to available options arena.py Main application Add game-specific CLI options","title":"Key Files Reference"},{"location":"adding_diff_game.html#example-workflow","text":"For a new fighting game: Import ROM and create starting state at match beginning Map health, position and round variables in data.json Define basic moves in a new config YAML file Create environment wrapper handling game-specific rewards Extend multiagent support for 2-player mode Update arena configuration and CLI Test with human controls first, then with agents By following this workflow, you can integrate most Stable-Retro compatible games into the Arena framework for AI research and evaluation.","title":"Example Workflow"},{"location":"arena.html","text":"Kane vs Abel Mortal Kombat II Arena Overview The Mortal Kombat II Arena is a framework for training, evaluating, and comparing different AI agents in the Mortal Kombat II environment. It supports various agent types including human players, DQN-based reinforcement learning agents, and behavior tree agents. System Architecture The arena implements a Model-View-Controller (MVC) architecture: Model ( EnvModel ) : Manages the game environment state and interactions View ( Renderer ) : Handles visualization of the game state Controller ( MortalKombatArena ) : Coordinates inputs, agent actions, and game updates Input Handler ( InputHandler ) : Processes keyboard inputs for human players Agent Types The arena supports the following agent types: Human : Controlled via keyboard inputs DQN : Deep Q-Network reinforcement learning agent Double DQN : Double Deep Q-Network for more stable learning Dueling DDQN : Dueling architecture with Double DQN for better value estimation BT : Behavior Tree-based agents that use predefined decision trees Command Line Interface Run the arena using the following command structure: python arena.py [options] Common Options Option Description --p1-type {human,dqn,double_dqn,dueling_ddqn,bt} Agent type for Player 1 --p1-model PATH Path to the model file for Player 1 (for RL agents) --p1-bt-file PATH Path to behavior tree YAML file for Player 1 (for BT agents) --p1-username NAME Username for Player 1 (for human players) --p2-type {human,dqn,double_dqn,dueling_ddqn,bt} Agent type for Player 2 --p2-model PATH Path to the model file for Player 2 (for RL agents) --p2-bt-file PATH Path to behavior tree YAML file for Player 2 (for BT agents) --p2-username NAME Username for Player 2 (for human players) --window-size WIDTH HEIGHT Window dimensions (default: 640 480) --fps FPS Target frames per second (default: 60) --switch Swap Player 1 and Player 2 positions Configuration Agent Configuration Each agent is configured through an AgentConfig object with these properties: - agent_type : The type of agent (\"human\", \"dqn\", \"double_dqn\", \"dueling_ddqn\", \"bt\") - model_path : Path to the model file (for RL agents) - bt_file_path : Path to the behavior tree file (for BT agents) - player_num : Player number (1 or 2) Arena Configuration The arena is configured through an ArenaConfig object with these properties: - game : Game name (\"MortalKombatII-Genesis\") - state : Initial game state (\"Level1.LiuKangVsJax.2P\") - players : Number of players (2) - window_size : Window dimensions for rendering - fps : Target frames per second - p1_agent : Agent configuration for Player 1 - p2_agent : Agent configuration for Player 2 ELO Rating System The arena includes an ELO rating system to track and compare agent performance: Each agent type has its own rating tracked in elo_ratings.json Ratings are updated after each match based on game outcome Default starting rating is 1500 Human players are tracked by their usernames AI agents are tracked by their uppercase agent types Behavior Trees BT agents use YAML files to define their decision trees: Each BT file defines a tree of nodes (Selectors, Sequences, Conditions, Actions) Conditions map to functions in the ConditionsProvider class Actions map to game inputs defined in env_config.yaml Example BT Structure node: type: Selector name: \"Root\" children: - type: Sequence name: \"Attack Sequence\" children: - type: Condition name: \"Is Close\" properties: condition: \"is_close_to_enemy\" - type: Action name: \"Attack\" properties: action_id: \"A\" frames_needed: 5 Examples Human vs. Double DQN python arena.py \\ --p1-type human \\ --p2-type double_dqn \\ --p2-model \"models/kane/DDQN_4M_with_lrSchedule_frameskip_24_actions.zip\" Dueling DQN vs. Double DQN python arena.py \\ --p1-type dueling_ddqn \\ --p1-model \"models/DuelingDQN_without_curriculum_4M_VeryEasyVsJax\" \\ --p2-type double_dqn \\ --p2-model \"models/DoubleDQN_without_curriculum_4M_VeryEasyVsJax_Exp_A\" BT vs. BT with Custom Trees python arena.py \\ --p1-type bt \\ --p1-bt-file \"bt_files/aggressive.yaml\" \\ --p2-type bt \\ --p2-bt-file \"bt_files/defensive.yaml\" Switching Player Positions python arena.py \\ --p1-type human \\ --p2-type double_dqn \\ --p2-model \"models/kane/model.zip\" \\ --switch Controls Player 1 Controls Arrow Keys : Movement Z : A button X : B button C : C button Enter : Start button Player 2 Controls W, A, S, D : Movement T : A button Y : B button U : C button Enter : Start button","title":"Home"},{"location":"arena.html#kane-vs-abel-mortal-kombat-ii-arena","text":"","title":"Kane vs Abel Mortal Kombat II Arena"},{"location":"arena.html#overview","text":"The Mortal Kombat II Arena is a framework for training, evaluating, and comparing different AI agents in the Mortal Kombat II environment. It supports various agent types including human players, DQN-based reinforcement learning agents, and behavior tree agents.","title":"Overview"},{"location":"arena.html#system-architecture","text":"The arena implements a Model-View-Controller (MVC) architecture: Model ( EnvModel ) : Manages the game environment state and interactions View ( Renderer ) : Handles visualization of the game state Controller ( MortalKombatArena ) : Coordinates inputs, agent actions, and game updates Input Handler ( InputHandler ) : Processes keyboard inputs for human players","title":"System Architecture"},{"location":"arena.html#agent-types","text":"The arena supports the following agent types: Human : Controlled via keyboard inputs DQN : Deep Q-Network reinforcement learning agent Double DQN : Double Deep Q-Network for more stable learning Dueling DDQN : Dueling architecture with Double DQN for better value estimation BT : Behavior Tree-based agents that use predefined decision trees","title":"Agent Types"},{"location":"arena.html#command-line-interface","text":"Run the arena using the following command structure: python arena.py [options]","title":"Command Line Interface"},{"location":"arena.html#common-options","text":"Option Description --p1-type {human,dqn,double_dqn,dueling_ddqn,bt} Agent type for Player 1 --p1-model PATH Path to the model file for Player 1 (for RL agents) --p1-bt-file PATH Path to behavior tree YAML file for Player 1 (for BT agents) --p1-username NAME Username for Player 1 (for human players) --p2-type {human,dqn,double_dqn,dueling_ddqn,bt} Agent type for Player 2 --p2-model PATH Path to the model file for Player 2 (for RL agents) --p2-bt-file PATH Path to behavior tree YAML file for Player 2 (for BT agents) --p2-username NAME Username for Player 2 (for human players) --window-size WIDTH HEIGHT Window dimensions (default: 640 480) --fps FPS Target frames per second (default: 60) --switch Swap Player 1 and Player 2 positions","title":"Common Options"},{"location":"arena.html#configuration","text":"","title":"Configuration"},{"location":"arena.html#agent-configuration","text":"Each agent is configured through an AgentConfig object with these properties: - agent_type : The type of agent (\"human\", \"dqn\", \"double_dqn\", \"dueling_ddqn\", \"bt\") - model_path : Path to the model file (for RL agents) - bt_file_path : Path to the behavior tree file (for BT agents) - player_num : Player number (1 or 2)","title":"Agent Configuration"},{"location":"arena.html#arena-configuration","text":"The arena is configured through an ArenaConfig object with these properties: - game : Game name (\"MortalKombatII-Genesis\") - state : Initial game state (\"Level1.LiuKangVsJax.2P\") - players : Number of players (2) - window_size : Window dimensions for rendering - fps : Target frames per second - p1_agent : Agent configuration for Player 1 - p2_agent : Agent configuration for Player 2","title":"Arena Configuration"},{"location":"arena.html#elo-rating-system","text":"The arena includes an ELO rating system to track and compare agent performance: Each agent type has its own rating tracked in elo_ratings.json Ratings are updated after each match based on game outcome Default starting rating is 1500 Human players are tracked by their usernames AI agents are tracked by their uppercase agent types","title":"ELO Rating System"},{"location":"arena.html#behavior-trees","text":"BT agents use YAML files to define their decision trees: Each BT file defines a tree of nodes (Selectors, Sequences, Conditions, Actions) Conditions map to functions in the ConditionsProvider class Actions map to game inputs defined in env_config.yaml","title":"Behavior Trees"},{"location":"arena.html#example-bt-structure","text":"node: type: Selector name: \"Root\" children: - type: Sequence name: \"Attack Sequence\" children: - type: Condition name: \"Is Close\" properties: condition: \"is_close_to_enemy\" - type: Action name: \"Attack\" properties: action_id: \"A\" frames_needed: 5","title":"Example BT Structure"},{"location":"arena.html#examples","text":"","title":"Examples"},{"location":"arena.html#human-vs-double-dqn","text":"python arena.py \\ --p1-type human \\ --p2-type double_dqn \\ --p2-model \"models/kane/DDQN_4M_with_lrSchedule_frameskip_24_actions.zip\"","title":"Human vs. Double DQN"},{"location":"arena.html#dueling-dqn-vs-double-dqn","text":"python arena.py \\ --p1-type dueling_ddqn \\ --p1-model \"models/DuelingDQN_without_curriculum_4M_VeryEasyVsJax\" \\ --p2-type double_dqn \\ --p2-model \"models/DoubleDQN_without_curriculum_4M_VeryEasyVsJax_Exp_A\"","title":"Dueling DQN vs. Double DQN"},{"location":"arena.html#bt-vs-bt-with-custom-trees","text":"python arena.py \\ --p1-type bt \\ --p1-bt-file \"bt_files/aggressive.yaml\" \\ --p2-type bt \\ --p2-bt-file \"bt_files/defensive.yaml\"","title":"BT vs. BT with Custom Trees"},{"location":"arena.html#switching-player-positions","text":"python arena.py \\ --p1-type human \\ --p2-type double_dqn \\ --p2-model \"models/kane/model.zip\" \\ --switch","title":"Switching Player Positions"},{"location":"arena.html#controls","text":"","title":"Controls"},{"location":"arena.html#player-1-controls","text":"Arrow Keys : Movement Z : A button X : B button C : C button Enter : Start button","title":"Player 1 Controls"},{"location":"arena.html#player-2-controls","text":"W, A, S, D : Movement T : A button Y : B button U : C button Enter : Start button","title":"Player 2 Controls"},{"location":"behaviour_tree.html","text":"Creating Custom Behavior Trees This document explains how to create your own behavior trees for AI agents in the Kane vs Abel Mortal Kombat II environment. Introduction to Behavior Trees Behavior Trees (BTs) are a hierarchical structure of nodes that control decision making for AI agents. They offer several advantages: Modular, reusable behavior components Visual representation of decision logic Readable and maintainable AI designs Effective for implementing complex game AI Behavior Tree Structure A behavior tree is defined in YAML format and consists of different node types arranged in a hierarchical structure: node: type: Selector # Root node type name: \"Root\" # Node name for debugging children: # Child nodes - type: Sequence name: \"Attack Sequence\" children: # More nodes... Node Types Selector Node Executes children from left to right until one succeeds. If all fail, the selector fails. type: Selector name: \"Choose Attack\" children: - type: Action name: \"High Attack\" properties: action_id: A frames_needed: 3 - type: Action name: \"Low Attack\" properties: action_id: DOWN_A frames_needed: 3 Sequence Node Executes children from left to right until all succeed or one fails. If any fail, the sequence fails. type: Sequence name: \"Jump and Attack\" children: - type: Action name: \"Jump\" properties: action_id: UP frames_needed: 2 - type: Action name: \"Attack\" properties: action_id: A frames_needed: 1 Condition Node Tests a condition and returns success or failure based on the result. type: Condition name: \"Is Enemy Close\" properties: condition: \"is_close_to_enemy\" # Must match a function in ConditionsProvider Action Node Performs a game action for a specified number of frames. type: Action name: \"Block\" properties: action_id: B # Must match an action in action_map frames_needed: 5 # Number of frames to hold the action Creating Your BT File File Structure Create a YAML file with a single root node: node: type: Selector name: \"Fighter Behavior\" children: # Add your behavior tree nodes here Available Conditions The following conditions are available from ConditionsProvider : Condition Name Function Description is_enemy_to_the_right Returns true when the enemy is to the right of the player Used for directional awareness is_enemy_to_the_left Returns true when the enemy is to the left of the player Used for directional awareness is_close_to_enemy Returns true when the enemy is within 50 units Used for close-range actions is_long_range_enemy Returns true when the enemy is beyond 50 units Used for ranged attacks is_medium_range_enemy Returns true when the enemy is between 50-120 units Good for jump-in attacks Available Actions Actions are defined in env_config.yaml and mapped to names in the ActionGenerator. Common actions include: Action Name Description Button Combination NEUTRAL No buttons pressed None LEFT Move left LEFT RIGHT Move right RIGHT UP Jump UP DOWN Crouch DOWN A A button A B B button B LEFT_DOWN Crouch + move left LEFT+DOWN RIGHT_DOWN Crouch + move right RIGHT+DOWN UP_A Jump + A UP+A DOWN_B Crouch + B DOWN+B Complete Example: Aggressive Fighter node: type: Selector name: \"Aggressive Fighter\" children: # Close Range Attack - type: Sequence name: \"Close Range Attack\" children: - type: Condition name: \"Enemy is Close\" properties: condition: \"is_close_to_enemy\" - type: Selector name: \"Choose Close Attack\" children: - type: Action name: \"Leg Sweep\" properties: action_id: DOWN_B frames_needed: 3 - type: Action name: \"Low Punch\" properties: action_id: DOWN_A frames_needed: 3 # Medium Range Jump-In - type: Sequence name: \"Jump Attack Approach\" children: - type: Condition name: \"Enemy is Medium Range\" properties: condition: \"is_medium_range_enemy\" - type: Selector name: \"Jump Direction\" children: - type: Sequence name: \"Jump Right Attack\" children: - type: Condition name: \"Enemy to Right\" properties: condition: \"is_enemy_to_the_right\" - type: Action name: \"Jump Forward Attack\" properties: action_id: RIGHT_UP_A frames_needed: 5 - type: Sequence name: \"Jump Left Attack\" children: - type: Condition name: \"Enemy to Left\" properties: condition: \"is_enemy_to_the_left\" - type: Action name: \"Jump Forward Attack\" properties: action_id: LEFT_UP_A frames_needed: 5 # Long Range Fireball - type: Sequence name: \"Long Range Attack\" children: - type: Condition name: \"Enemy is Far\" properties: condition: \"is_long_range_enemy\" - type: Action name: \"Fireball Special\" properties: action_id: RIGHT_UP_B frames_needed: 7 # Approach - Movement - type: Selector name: \"Movement\" children: - type: Sequence name: \"Move Right\" children: - type: Condition name: \"Enemy to Right\" properties: condition: \"is_enemy_to_the_right\" - type: Action name: \"Move Right\" properties: action_id: RIGHT frames_needed: 3 - type: Sequence name: \"Move Left\" children: - type: Condition name: \"Enemy to Left\" properties: condition: \"is_enemy_to_the_left\" - type: Action name: \"Move Left\" properties: action_id: LEFT frames_needed: 3 Best Practices Prioritize actions properly : Place the most important behaviors at the top of selectors Balance reactivity : Don't set frames_needed too high or your agent will be unresponsive Test incrementally : Start with a simple BT and gradually add complexity Use meaningful node names : They help when debugging behaviors Consider opponent position : Always check enemy position before directional actions Add fallback behaviors : Ensure your tree has default actions if no conditions match Debugging BTs Check action names : Ensure they match exactly with keys in action_map Verify conditions : Make sure condition names match functions in ConditionsProvider Start simple : Begin with a few nodes and gradually expand Test against stationary opponents first to verify behavior Watch for indentation errors in YAML that may break the tree structure Using Your Custom BT Run the arena with your custom BT file: python arena.py --p1-type bt --p1-bt-file path/to/your_bt.yaml Or for player 2: python arena.py --p2-type bt --p2-bt-file path/to/your_bt.yaml Further Customization To add custom conditions: 1. Add your function to the ConditionsProvider class in conditions.py 2. Use your new condition name in the BT YAML file","title":"Behaviour Tree"},{"location":"behaviour_tree.html#creating-custom-behavior-trees","text":"This document explains how to create your own behavior trees for AI agents in the Kane vs Abel Mortal Kombat II environment.","title":"Creating Custom Behavior Trees"},{"location":"behaviour_tree.html#introduction-to-behavior-trees","text":"Behavior Trees (BTs) are a hierarchical structure of nodes that control decision making for AI agents. They offer several advantages: Modular, reusable behavior components Visual representation of decision logic Readable and maintainable AI designs Effective for implementing complex game AI","title":"Introduction to Behavior Trees"},{"location":"behaviour_tree.html#behavior-tree-structure","text":"A behavior tree is defined in YAML format and consists of different node types arranged in a hierarchical structure: node: type: Selector # Root node type name: \"Root\" # Node name for debugging children: # Child nodes - type: Sequence name: \"Attack Sequence\" children: # More nodes...","title":"Behavior Tree Structure"},{"location":"behaviour_tree.html#node-types","text":"","title":"Node Types"},{"location":"behaviour_tree.html#selector-node","text":"Executes children from left to right until one succeeds. If all fail, the selector fails. type: Selector name: \"Choose Attack\" children: - type: Action name: \"High Attack\" properties: action_id: A frames_needed: 3 - type: Action name: \"Low Attack\" properties: action_id: DOWN_A frames_needed: 3","title":"Selector Node"},{"location":"behaviour_tree.html#sequence-node","text":"Executes children from left to right until all succeed or one fails. If any fail, the sequence fails. type: Sequence name: \"Jump and Attack\" children: - type: Action name: \"Jump\" properties: action_id: UP frames_needed: 2 - type: Action name: \"Attack\" properties: action_id: A frames_needed: 1","title":"Sequence Node"},{"location":"behaviour_tree.html#condition-node","text":"Tests a condition and returns success or failure based on the result. type: Condition name: \"Is Enemy Close\" properties: condition: \"is_close_to_enemy\" # Must match a function in ConditionsProvider","title":"Condition Node"},{"location":"behaviour_tree.html#action-node","text":"Performs a game action for a specified number of frames. type: Action name: \"Block\" properties: action_id: B # Must match an action in action_map frames_needed: 5 # Number of frames to hold the action","title":"Action Node"},{"location":"behaviour_tree.html#creating-your-bt-file","text":"","title":"Creating Your BT File"},{"location":"behaviour_tree.html#file-structure","text":"Create a YAML file with a single root node: node: type: Selector name: \"Fighter Behavior\" children: # Add your behavior tree nodes here","title":"File Structure"},{"location":"behaviour_tree.html#available-conditions","text":"The following conditions are available from ConditionsProvider : Condition Name Function Description is_enemy_to_the_right Returns true when the enemy is to the right of the player Used for directional awareness is_enemy_to_the_left Returns true when the enemy is to the left of the player Used for directional awareness is_close_to_enemy Returns true when the enemy is within 50 units Used for close-range actions is_long_range_enemy Returns true when the enemy is beyond 50 units Used for ranged attacks is_medium_range_enemy Returns true when the enemy is between 50-120 units Good for jump-in attacks","title":"Available Conditions"},{"location":"behaviour_tree.html#available-actions","text":"Actions are defined in env_config.yaml and mapped to names in the ActionGenerator. Common actions include: Action Name Description Button Combination NEUTRAL No buttons pressed None LEFT Move left LEFT RIGHT Move right RIGHT UP Jump UP DOWN Crouch DOWN A A button A B B button B LEFT_DOWN Crouch + move left LEFT+DOWN RIGHT_DOWN Crouch + move right RIGHT+DOWN UP_A Jump + A UP+A DOWN_B Crouch + B DOWN+B","title":"Available Actions"},{"location":"behaviour_tree.html#complete-example-aggressive-fighter","text":"node: type: Selector name: \"Aggressive Fighter\" children: # Close Range Attack - type: Sequence name: \"Close Range Attack\" children: - type: Condition name: \"Enemy is Close\" properties: condition: \"is_close_to_enemy\" - type: Selector name: \"Choose Close Attack\" children: - type: Action name: \"Leg Sweep\" properties: action_id: DOWN_B frames_needed: 3 - type: Action name: \"Low Punch\" properties: action_id: DOWN_A frames_needed: 3 # Medium Range Jump-In - type: Sequence name: \"Jump Attack Approach\" children: - type: Condition name: \"Enemy is Medium Range\" properties: condition: \"is_medium_range_enemy\" - type: Selector name: \"Jump Direction\" children: - type: Sequence name: \"Jump Right Attack\" children: - type: Condition name: \"Enemy to Right\" properties: condition: \"is_enemy_to_the_right\" - type: Action name: \"Jump Forward Attack\" properties: action_id: RIGHT_UP_A frames_needed: 5 - type: Sequence name: \"Jump Left Attack\" children: - type: Condition name: \"Enemy to Left\" properties: condition: \"is_enemy_to_the_left\" - type: Action name: \"Jump Forward Attack\" properties: action_id: LEFT_UP_A frames_needed: 5 # Long Range Fireball - type: Sequence name: \"Long Range Attack\" children: - type: Condition name: \"Enemy is Far\" properties: condition: \"is_long_range_enemy\" - type: Action name: \"Fireball Special\" properties: action_id: RIGHT_UP_B frames_needed: 7 # Approach - Movement - type: Selector name: \"Movement\" children: - type: Sequence name: \"Move Right\" children: - type: Condition name: \"Enemy to Right\" properties: condition: \"is_enemy_to_the_right\" - type: Action name: \"Move Right\" properties: action_id: RIGHT frames_needed: 3 - type: Sequence name: \"Move Left\" children: - type: Condition name: \"Enemy to Left\" properties: condition: \"is_enemy_to_the_left\" - type: Action name: \"Move Left\" properties: action_id: LEFT frames_needed: 3","title":"Complete Example: Aggressive Fighter"},{"location":"behaviour_tree.html#best-practices","text":"Prioritize actions properly : Place the most important behaviors at the top of selectors Balance reactivity : Don't set frames_needed too high or your agent will be unresponsive Test incrementally : Start with a simple BT and gradually add complexity Use meaningful node names : They help when debugging behaviors Consider opponent position : Always check enemy position before directional actions Add fallback behaviors : Ensure your tree has default actions if no conditions match","title":"Best Practices"},{"location":"behaviour_tree.html#debugging-bts","text":"Check action names : Ensure they match exactly with keys in action_map Verify conditions : Make sure condition names match functions in ConditionsProvider Start simple : Begin with a few nodes and gradually expand Test against stationary opponents first to verify behavior Watch for indentation errors in YAML that may break the tree structure","title":"Debugging BTs"},{"location":"behaviour_tree.html#using-your-custom-bt","text":"Run the arena with your custom BT file: python arena.py --p1-type bt --p1-bt-file path/to/your_bt.yaml Or for player 2: python arena.py --p2-type bt --p2-bt-file path/to/your_bt.yaml","title":"Using Your Custom BT"},{"location":"behaviour_tree.html#further-customization","text":"To add custom conditions: 1. Add your function to the ConditionsProvider class in conditions.py 2. Use your new condition name in the BT YAML file","title":"Further Customization"},{"location":"curriculum_learning.html","text":"Curriculum Learning for Mortal Kombat II Curriculum learning is an optional training strategy that gradually increases the difficulty of training scenarios as the agent improves. This approach can lead to more efficient learning and better final performance, especially for complex fighting game environments like Mortal Kombat II. What is Curriculum Learning? Curriculum learning mimics human learning by starting with simpler tasks and gradually increasing complexity. In the context of Mortal Kombat II training: The agent begins fighting against very easy opponents with limited move sets As performance improves, the agent advances to more challenging opponents Eventually, the agent faces opponents with full move sets and higher difficulty settings Benefits of Curriculum Learning Faster initial learning : Agents learn basic mechanics more quickly Higher final performance : Gradual progression helps avoid local optima More robust behaviors : Exposure to diverse scenarios builds generalization Reduced training time : More efficient exploration of the state space Implementing Curriculum Learning Kane vs Abel framework implements curriculum learning through tiered state lists and a dedicated callback: 1. Define State Tiers First, define tiers of game states with increasing difficulty: tier1_states = [\"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\"] tier2_states = [ \"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\", \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\" ] tier3_states = [ \"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\", \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\", \"VeryEasy.LiuKang-06\", \"VeryEasy.LiuKang-07\", \"VeryEasy.LiuKang-08\" ] tiered_states = [tier1_states, tier2_states, tier3_states] 2. Create the CurriculumCallback Enable the curriculum learning callback in your training script: curriculum_callback = CurriculumCallback( vec_env=venv, tiered_states=tiered_states, verbose=1, buffer_size=100 ) 3. Add to Callback List Include the curriculum callback in your model's training: callback_list = CallbackList([eval_callback, curriculum_callback]) model.learn( total_timesteps=TOTAL_TIMESTEPS, reset_num_timesteps=True, callback=callback_list ) How the Curriculum Callback Works The CurriculumCallback in mk_ai.callbacks.curriculum manages the progression through training tiers: Initialization : Sets up with tier 1 states and creates a reward buffer Performance Tracking : Monitors the agent's average reward over recent episodes Tier Advancement : When average reward exceeds thresholds, advances to next tier: Tier 1 \u2192 Tier 2: When average reward > 50 Tier 2 \u2192 Tier 3: When average reward > 150 Tier 3 \u2192 Tier 4 (if defined): When average reward > 250 Environment Update : When advancing tiers, updates all parallel environments with new state sets Key Parameters for CurriculumCallback vec_env : The vectorized environment to update tiered_states : List of state lists for each tier buffer_size : Number of episode rewards to average (default: 20) verbose : Logging verbosity level Customizing the Curriculum Custom Advancement Thresholds If you want different thresholds for curriculum advancement, modify the _on_step method in CurriculumCallback : def _on_step(self) -> bool: # ...existing code... # Custom thresholds if self.current_tier_idx == 0 and avg_reward > 75: # Changed from 50 self.current_tier_idx = 1 print(f\"[Callback] Switching to Tier 2, avg_reward={avg_reward:.2f}\") self._update_env_states() elif self.current_tier_idx == 1 and avg_reward > 200: # Changed from 150 self.current_tier_idx = 2 print(f\"[Callback] Switching to Tier 3, avg_reward={avg_reward:.2f}\") self._update_env_states() # ...existing code... Custom State Progression You can define your own progression strategy by creating custom tier lists: # Character-based progression (same character, increasing difficulty) tier1 = [\"VeryEasy.LiuKang-01\", \"VeryEasy.LiuKang-02\"] tier2 = [\"Easy.LiuKang-01\", \"Easy.LiuKang-02\"] tier3 = [\"Medium.LiuKang-01\", \"Medium.LiuKang-02\"] # Or opponent-based progression (increasing variety) tier1 = [\"VeryEasy.LiuKangVsJax\", \"VeryEasy.LiuKangVsBaraka\"] tier2 = [\"VeryEasy.LiuKangVsJax\", \"VeryEasy.LiuKangVsBaraka\", \"VeryEasy.LiuKangVsReptile\", \"VeryEasy.LiuKangVsKitana\"] tier3 = [\"Easy.LiuKangVsJax\", \"Easy.LiuKangVsBaraka\", \"Easy.LiuKangVsReptile\", \"Easy.LiuKangVsKitana\"] Example: Full Training with Curriculum Learning Here's a complete example of setting up curriculum learning in a training script: from mk_ai.callbacks import CurriculumCallback, CustomEvalCallback from stable_baselines3.common.callbacks import CallbackList # Define curriculum tiers tier1_states = [\"VeryEasy.LiuKang-01\", \"VeryEasy.LiuKang-02\"] tier2_states = [\"Easy.LiuKang-01\", \"Easy.LiuKang-02\", \"Easy.LiuKang-03\"] tier3_states = [\"Medium.LiuKang-01\", \"Medium.LiuKang-02\"] tiered_states = [tier1_states, tier2_states, tier3_states] # Create vectorized environment (start with tier 1) venv = SubprocVecEnv([make_env(tier1_states) for _ in range(8)]) stacked_env = VecFrameStack(venv, n_stack=4) # Create model model = DuelingDoubleDQN( env=stacked_env, verbose=1, device=\"cuda\", # ...other parameters... ) # Create curriculum callback curriculum_callback = CurriculumCallback( vec_env=venv, tiered_states=tiered_states, verbose=1, buffer_size=100 ) # Create evaluation callback eval_callback = CustomEvalCallback( # ...evaluation parameters... ) # Create callback list callbacks = CallbackList([eval_callback, curriculum_callback]) # Train with curriculum learning model.learn( total_timesteps=16_000_000, callback=callbacks ) When to Use Curriculum Learning Curriculum learning is most beneficial: - When training from scratch in complex environments - When the agent struggles to learn meaningful behaviors with random initialization - When you have a clear progression of difficulty levels available It may be less necessary: - When fine-tuning pre-trained models - In simple environments where learning is already efficient - When computational resources are severely limited (adds overhead) Monitoring Curriculum Progress During training, the curriculum callback will output log messages when it advances to a new tier: [Callback] Switching to Tier 2, avg_reward=52.38 ... [Callback] Switching to Tier 3, avg_reward=157.92 You can monitor this progress along with other training metrics in TensorBoard.","title":"Curriculum Learning"},{"location":"curriculum_learning.html#curriculum-learning-for-mortal-kombat-ii","text":"Curriculum learning is an optional training strategy that gradually increases the difficulty of training scenarios as the agent improves. This approach can lead to more efficient learning and better final performance, especially for complex fighting game environments like Mortal Kombat II.","title":"Curriculum Learning for Mortal Kombat II"},{"location":"curriculum_learning.html#what-is-curriculum-learning","text":"Curriculum learning mimics human learning by starting with simpler tasks and gradually increasing complexity. In the context of Mortal Kombat II training: The agent begins fighting against very easy opponents with limited move sets As performance improves, the agent advances to more challenging opponents Eventually, the agent faces opponents with full move sets and higher difficulty settings","title":"What is Curriculum Learning?"},{"location":"curriculum_learning.html#benefits-of-curriculum-learning","text":"Faster initial learning : Agents learn basic mechanics more quickly Higher final performance : Gradual progression helps avoid local optima More robust behaviors : Exposure to diverse scenarios builds generalization Reduced training time : More efficient exploration of the state space","title":"Benefits of Curriculum Learning"},{"location":"curriculum_learning.html#implementing-curriculum-learning","text":"Kane vs Abel framework implements curriculum learning through tiered state lists and a dedicated callback:","title":"Implementing Curriculum Learning"},{"location":"curriculum_learning.html#1-define-state-tiers","text":"First, define tiers of game states with increasing difficulty: tier1_states = [\"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\"] tier2_states = [ \"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\", \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\" ] tier3_states = [ \"Level1.LiuKangVsJax\", \"VeryEasy.LiuKang-02\", \"VeryEasy.LiuKang-03\", \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\", \"VeryEasy.LiuKang-06\", \"VeryEasy.LiuKang-07\", \"VeryEasy.LiuKang-08\" ] tiered_states = [tier1_states, tier2_states, tier3_states]","title":"1. Define State Tiers"},{"location":"curriculum_learning.html#2-create-the-curriculumcallback","text":"Enable the curriculum learning callback in your training script: curriculum_callback = CurriculumCallback( vec_env=venv, tiered_states=tiered_states, verbose=1, buffer_size=100 )","title":"2. Create the CurriculumCallback"},{"location":"curriculum_learning.html#3-add-to-callback-list","text":"Include the curriculum callback in your model's training: callback_list = CallbackList([eval_callback, curriculum_callback]) model.learn( total_timesteps=TOTAL_TIMESTEPS, reset_num_timesteps=True, callback=callback_list )","title":"3. Add to Callback List"},{"location":"curriculum_learning.html#how-the-curriculum-callback-works","text":"The CurriculumCallback in mk_ai.callbacks.curriculum manages the progression through training tiers: Initialization : Sets up with tier 1 states and creates a reward buffer Performance Tracking : Monitors the agent's average reward over recent episodes Tier Advancement : When average reward exceeds thresholds, advances to next tier: Tier 1 \u2192 Tier 2: When average reward > 50 Tier 2 \u2192 Tier 3: When average reward > 150 Tier 3 \u2192 Tier 4 (if defined): When average reward > 250 Environment Update : When advancing tiers, updates all parallel environments with new state sets","title":"How the Curriculum Callback Works"},{"location":"curriculum_learning.html#key-parameters-for-curriculumcallback","text":"vec_env : The vectorized environment to update tiered_states : List of state lists for each tier buffer_size : Number of episode rewards to average (default: 20) verbose : Logging verbosity level","title":"Key Parameters for CurriculumCallback"},{"location":"curriculum_learning.html#customizing-the-curriculum","text":"","title":"Customizing the Curriculum"},{"location":"curriculum_learning.html#custom-advancement-thresholds","text":"If you want different thresholds for curriculum advancement, modify the _on_step method in CurriculumCallback : def _on_step(self) -> bool: # ...existing code... # Custom thresholds if self.current_tier_idx == 0 and avg_reward > 75: # Changed from 50 self.current_tier_idx = 1 print(f\"[Callback] Switching to Tier 2, avg_reward={avg_reward:.2f}\") self._update_env_states() elif self.current_tier_idx == 1 and avg_reward > 200: # Changed from 150 self.current_tier_idx = 2 print(f\"[Callback] Switching to Tier 3, avg_reward={avg_reward:.2f}\") self._update_env_states() # ...existing code...","title":"Custom Advancement Thresholds"},{"location":"curriculum_learning.html#custom-state-progression","text":"You can define your own progression strategy by creating custom tier lists: # Character-based progression (same character, increasing difficulty) tier1 = [\"VeryEasy.LiuKang-01\", \"VeryEasy.LiuKang-02\"] tier2 = [\"Easy.LiuKang-01\", \"Easy.LiuKang-02\"] tier3 = [\"Medium.LiuKang-01\", \"Medium.LiuKang-02\"] # Or opponent-based progression (increasing variety) tier1 = [\"VeryEasy.LiuKangVsJax\", \"VeryEasy.LiuKangVsBaraka\"] tier2 = [\"VeryEasy.LiuKangVsJax\", \"VeryEasy.LiuKangVsBaraka\", \"VeryEasy.LiuKangVsReptile\", \"VeryEasy.LiuKangVsKitana\"] tier3 = [\"Easy.LiuKangVsJax\", \"Easy.LiuKangVsBaraka\", \"Easy.LiuKangVsReptile\", \"Easy.LiuKangVsKitana\"]","title":"Custom State Progression"},{"location":"curriculum_learning.html#example-full-training-with-curriculum-learning","text":"Here's a complete example of setting up curriculum learning in a training script: from mk_ai.callbacks import CurriculumCallback, CustomEvalCallback from stable_baselines3.common.callbacks import CallbackList # Define curriculum tiers tier1_states = [\"VeryEasy.LiuKang-01\", \"VeryEasy.LiuKang-02\"] tier2_states = [\"Easy.LiuKang-01\", \"Easy.LiuKang-02\", \"Easy.LiuKang-03\"] tier3_states = [\"Medium.LiuKang-01\", \"Medium.LiuKang-02\"] tiered_states = [tier1_states, tier2_states, tier3_states] # Create vectorized environment (start with tier 1) venv = SubprocVecEnv([make_env(tier1_states) for _ in range(8)]) stacked_env = VecFrameStack(venv, n_stack=4) # Create model model = DuelingDoubleDQN( env=stacked_env, verbose=1, device=\"cuda\", # ...other parameters... ) # Create curriculum callback curriculum_callback = CurriculumCallback( vec_env=venv, tiered_states=tiered_states, verbose=1, buffer_size=100 ) # Create evaluation callback eval_callback = CustomEvalCallback( # ...evaluation parameters... ) # Create callback list callbacks = CallbackList([eval_callback, curriculum_callback]) # Train with curriculum learning model.learn( total_timesteps=16_000_000, callback=callbacks )","title":"Example: Full Training with Curriculum Learning"},{"location":"curriculum_learning.html#when-to-use-curriculum-learning","text":"Curriculum learning is most beneficial: - When training from scratch in complex environments - When the agent struggles to learn meaningful behaviors with random initialization - When you have a clear progression of difficulty levels available It may be less necessary: - When fine-tuning pre-trained models - In simple environments where learning is already efficient - When computational resources are severely limited (adds overhead)","title":"When to Use Curriculum Learning"},{"location":"curriculum_learning.html#monitoring-curriculum-progress","text":"During training, the curriculum callback will output log messages when it advances to a new tier: [Callback] Switching to Tier 2, avg_reward=52.38 ... [Callback] Switching to Tier 3, avg_reward=157.92 You can monitor this progress along with other training metrics in TensorBoard.","title":"Monitoring Curriculum Progress"},{"location":"dqn.html","text":"Deep Q-Learning in Kane vs Abel Mortal Kombat II This document explains the different Deep Q-Learning variants implemented in the project, their key differences, and how they work. Basic DQN Algorithm Core Components: Q-Network : Neural network that approximates the Q-value function Replay Buffer : Stores experience tuples (state, action, reward, next_state, done) Target Network : Copy of Q-network for stable target calculation Exploration Policy : Typically epsilon-greedy for balancing exploration and exploitation Training Loop: Initialize Environment and Replay Buffer : Start with an empty replay buffer to store experiences Initialize Q-Networks : Policy network (for action selection) Target network (periodically updated copy) For each episode : Reset environment to initial state For each step until episode ends: Select action using epsilon-greedy policy Execute action, observe reward and next state Store experience in replay buffer Sample mini-batch from replay buffer Compute target Q-values: $y = r + \\gamma \\max_{a'} Q_{target}(s', a')$ Update Q-network by minimizing loss: $L = \\frac{1}{N} \\sum (y - Q(s,a))^2$ Periodically update target network Implemented Variants 1. Double DQN Implementation : mk_ai.agents.DQN.double_dqn.py Key Improvement : Addresses overestimation bias in standard DQN How It Works : - Uses the online network to select actions - Uses the target network to evaluate those actions - Target calculation: $y = r + \\gamma Q_{target}(s', \\arg\\max_{a'} Q_{online}(s', a'))$ Code Highlight : # Select action with online network next_actions_online = th.argmax(next_q_values_online, dim=1) # Use target network to evaluate action next_q_values = th.gather(next_q_values, dim=1, index=next_actions_online.unsqueeze(1)).flatten() 2. Dueling DQN Implementation : dueling_dqn.py Key Improvement : Separates state value and action advantage estimation How It Works : - Splits the Q-network into two streams: - Value stream: estimates state value V(s) - Advantage stream: estimates action advantages A(s,a) - Combines them: $Q(s,a) = V(s) + (A(s,a) - \\frac{1}{|A|}\\sum_a A(s,a))$ Network Architecture : - Shared feature extractor (usually CNN for game images) - Separate value and advantage heads - Special aggregation layer to combine outputs 3. Dueling Double DQN Implementation : dueling_ddqn.py Key Improvement : Combines both Double DQN and Dueling architecture benefits How It Works : - Uses Dueling architecture (separate value and advantage streams) - Applies Double DQN update rule (decoupling action selection from evaluation) - Inherits from Double DQN but uses Dueling network architecture Network Structure : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Feature Layers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Value Stream \u2502 \u2502 Advantage Stream \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Q(s,a) = V(s) + A(s,a) - mean \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Comparing Performance In Mortal Kombat II gameplay, we've observed: Standard DQN : Tends to overestimate action values, leading to suboptimal policies Double DQN : More conservative value estimates, steadier learning Dueling DQN : Better at estimating state values independent of actions Dueling Double DQN : Best overall performance, combining the strengths of both approaches Using Different DQN Variants in the Arena The Arena system allows selecting different DQN variants: # Standard DQN python arena.py --p1-type dqn --p1-model models/dqn_model # Double DQN python arena.py --p1-type double_dqn --p1-model models/double_dqn_model # Dueling DQN (architecturally different but same update rule as DQN) python arena.py --p1-type dqn --p1-model models/dueling_dqn_model # Dueling Double DQN (both architectural and update rule changes) python arena.py --p1-type dueling_ddqn --p1-model models/dueling_double_dqn_model Implementation References Double DQN: Deep Reinforcement Learning with Double Q-Learning Dueling DQN: Dueling Network Architectures for Deep Reinforcement Learning","title":"DQN"},{"location":"dqn.html#deep-q-learning-in-kane-vs-abel-mortal-kombat-ii","text":"This document explains the different Deep Q-Learning variants implemented in the project, their key differences, and how they work.","title":"Deep Q-Learning in Kane vs Abel Mortal Kombat II"},{"location":"dqn.html#basic-dqn-algorithm","text":"","title":"Basic DQN Algorithm"},{"location":"dqn.html#core-components","text":"Q-Network : Neural network that approximates the Q-value function Replay Buffer : Stores experience tuples (state, action, reward, next_state, done) Target Network : Copy of Q-network for stable target calculation Exploration Policy : Typically epsilon-greedy for balancing exploration and exploitation","title":"Core Components:"},{"location":"dqn.html#training-loop","text":"Initialize Environment and Replay Buffer : Start with an empty replay buffer to store experiences Initialize Q-Networks : Policy network (for action selection) Target network (periodically updated copy) For each episode : Reset environment to initial state For each step until episode ends: Select action using epsilon-greedy policy Execute action, observe reward and next state Store experience in replay buffer Sample mini-batch from replay buffer Compute target Q-values: $y = r + \\gamma \\max_{a'} Q_{target}(s', a')$ Update Q-network by minimizing loss: $L = \\frac{1}{N} \\sum (y - Q(s,a))^2$ Periodically update target network","title":"Training Loop:"},{"location":"dqn.html#implemented-variants","text":"","title":"Implemented Variants"},{"location":"dqn.html#1-double-dqn","text":"Implementation : mk_ai.agents.DQN.double_dqn.py Key Improvement : Addresses overestimation bias in standard DQN How It Works : - Uses the online network to select actions - Uses the target network to evaluate those actions - Target calculation: $y = r + \\gamma Q_{target}(s', \\arg\\max_{a'} Q_{online}(s', a'))$ Code Highlight : # Select action with online network next_actions_online = th.argmax(next_q_values_online, dim=1) # Use target network to evaluate action next_q_values = th.gather(next_q_values, dim=1, index=next_actions_online.unsqueeze(1)).flatten()","title":"1. Double DQN"},{"location":"dqn.html#2-dueling-dqn","text":"Implementation : dueling_dqn.py Key Improvement : Separates state value and action advantage estimation How It Works : - Splits the Q-network into two streams: - Value stream: estimates state value V(s) - Advantage stream: estimates action advantages A(s,a) - Combines them: $Q(s,a) = V(s) + (A(s,a) - \\frac{1}{|A|}\\sum_a A(s,a))$ Network Architecture : - Shared feature extractor (usually CNN for game images) - Separate value and advantage heads - Special aggregation layer to combine outputs","title":"2. Dueling DQN"},{"location":"dqn.html#3-dueling-double-dqn","text":"Implementation : dueling_ddqn.py Key Improvement : Combines both Double DQN and Dueling architecture benefits How It Works : - Uses Dueling architecture (separate value and advantage streams) - Applies Double DQN update rule (decoupling action selection from evaluation) - Inherits from Double DQN but uses Dueling network architecture Network Structure : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Feature Layers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Value Stream \u2502 \u2502 Advantage Stream \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Q(s,a) = V(s) + A(s,a) - mean \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"3. Dueling Double DQN"},{"location":"dqn.html#comparing-performance","text":"In Mortal Kombat II gameplay, we've observed: Standard DQN : Tends to overestimate action values, leading to suboptimal policies Double DQN : More conservative value estimates, steadier learning Dueling DQN : Better at estimating state values independent of actions Dueling Double DQN : Best overall performance, combining the strengths of both approaches","title":"Comparing Performance"},{"location":"dqn.html#using-different-dqn-variants-in-the-arena","text":"The Arena system allows selecting different DQN variants: # Standard DQN python arena.py --p1-type dqn --p1-model models/dqn_model # Double DQN python arena.py --p1-type double_dqn --p1-model models/double_dqn_model # Dueling DQN (architecturally different but same update rule as DQN) python arena.py --p1-type dqn --p1-model models/dueling_dqn_model # Dueling Double DQN (both architectural and update rule changes) python arena.py --p1-type dueling_ddqn --p1-model models/dueling_double_dqn_model","title":"Using Different DQN Variants in the Arena"},{"location":"dqn.html#implementation-references","text":"Double DQN: Deep Reinforcement Learning with Double Q-Learning Dueling DQN: Dueling Network Architectures for Deep Reinforcement Learning","title":"Implementation References"},{"location":"human_input.html","text":"Human Agent Controls and Input Customization This document explains how the human agent works in Kane vs Abel Mortal Kombat II, how keyboard input is processed, and how to customize or extend the input system. Human Agent Overview The HumanAgent class provides an interface for human players to participate in matches through keyboard inputs. It translates key presses into game actions compatible with the Stable-Retro environment. Default Controls Player 1 Controls Action Default Key Movement UP \u2191 (Up Arrow) DOWN \u2193 (Down Arrow) LEFT \u2190 (Left Arrow) RIGHT \u2192 (Right Arrow) Attacks A Z B X C C START Enter MODE Tab Player 2 Controls Action Default Key Movement UP W DOWN S LEFT A RIGHT D Attacks A T B Y C U START Enter MODE Tab Input Processing Pipeline The input system follows this flow: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Key Press \u2502 \u2502 InputHandler \u2502 \u2502 HumanAgent \u2502 \u2502 Game \u2502 \u2502 (Keyboard) \u2502\u2500\u2500\u2500\u2500>\u2502 (Key Tracking) \u2502\u2500\u2500\u2500\u2500>\u2502 (Action \u2502\u2500\u2500\u2500\u2500>\u2502 Environment \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 Conversion) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key presses are detected by pyglet's event system InputHandler maintains sets of pressed keys for each player HumanAgent converts key presses to binary action arrays Actions are passed to the game environment How Input Works Input Detection The InputHandler class listens for key events on the game window: def on_key_press(self, symbol, modifiers): # Track which player's key was pressed if symbol in P1_KEY_MAP: self.p1_pressed.add(symbol) elif symbol in P2_KEY_MAP: self.p2_pressed.add(symbol) Key-to-Action Conversion The HumanAgent converts tracked key presses to game actions: def select_action(self, obs, info) -> int: # Convert pressed keys to binary action array action_array = [0] * len(self.buttons) for key in self.pressed_keys: if key in self.key_map: button = self.key_map[key] if button in self.buttons: action_array[self.buttons.index(button)] = 1 # Find matching action ID for action_id, arr in enumerate(self.action_mapping): if arr == action_array: return action_id return 0 # Return NEUTRAL if no match Customizing Key Mappings You can modify key mappings by editing the key map constants in the project: # In mk_ai/configs/__init__.py or similar file from pyglet.window import key as keycodes P1_KEY_MAP = { keycodes.UP: \"UP\", keycodes.DOWN: \"DOWN\", keycodes.LEFT: \"LEFT\", keycodes.RIGHT: \"RIGHT\", keycodes.Z: \"A\", keycodes.X: \"B\", keycodes.C: \"C\", # Add custom mappings here } P2_KEY_MAP = { keycodes.W: \"UP\", keycodes.S: \"DOWN\", keycodes.A: \"LEFT\", keycodes.D: \"RIGHT\", keycodes.T: \"A\", keycodes.Y: \"B\", keycodes.U: \"C\", # Add custom mappings here } Adding New Input Methods Gamepad Support To add gamepad support: Create a gamepad handler class that inherits from Agent : class GamepadAgent(Agent): def __init__(self, action_mapping, buttons, gamepad_id=0): self.action_mapping = action_mapping self.buttons = buttons self.gamepad_id = gamepad_id # Initialize gamepad library (e.g., using pyglet or pygame) self.gamepad = self.initialize_gamepad(gamepad_id) def initialize_gamepad(self, id): # Implementation depends on the gamepad library you're using pass def select_action(self, obs, info): # Read gamepad state button_states = self.read_gamepad_state() # Convert to action array similar to HumanAgent action_array = [0] * len(self.buttons) for button_name, is_pressed in button_states.items(): if is_pressed and button_name in self.buttons: action_array[self.buttons.index(button_name)] = 1 # Find matching action ID for action_id, arr in enumerate(self.action_mapping): if arr == action_array: return action_id return 0 Update AgentFactory to support the new agent type: elif config.agent_type == \"gamepad\": return GamepadAgent( action_mapping=action_mapping, buttons=buttons, gamepad_id=config.gamepad_id ) Network Input (Online Play) For networked multiplayer: Create a NetworkAgent class that receives inputs from a remote player Implement a simple socket server to relay player inputs Update the AgentFactory to support the new agent type Troubleshooting Input Issues Common Issues and Solutions Keys not responding: Check that the key mapping exists in P1_KEY_MAP or P2_KEY_MAP Verify the window has focus when keys are pressed Multiple keys not working together: Some keyboards have limited key rollover. Try pressing fewer keys simultaneously Check if the key combination exists in env_config.yaml Key stuck on: Ensure on_key_release is being called properly Add manual cleanup code to reset keys when losing window focus Example: Adding WASD Controls for Player 1 from pyglet.window import key as keycodes # Modified key mapping that uses WASD for P1 movement P1_KEY_MAP = { # WASD movement keycodes.W: \"UP\", keycodes.A: \"LEFT\", keycodes.S: \"DOWN\", keycodes.D: \"RIGHT\", # Attack buttons keycodes.J: \"A\", keycodes.K: \"B\", keycodes.L: \"C\", keycodes.ENTER: \"START\", keycodes.SPACE: \"MODE\", } Advanced: Creating Custom Input Handlers For specialized input devices or complex control schemes, you can create custom input handlers: Create a new class that inherits from InputHandler Override the key event methods to support your devices Update your custom HumanAgent to work with your handler class CustomInputHandler(InputHandler): def __init__(self, window): super().__init__(window) # Initialize your custom input devices def setup_window_events(self): super().setup_window_events() # Add custom event handlers def update(self): # Poll custom input devices and update key states pass Then modify your game loop to use this custom handler. Similar code found with 1 license type","title":"Human Input"},{"location":"human_input.html#human-agent-controls-and-input-customization","text":"This document explains how the human agent works in Kane vs Abel Mortal Kombat II, how keyboard input is processed, and how to customize or extend the input system.","title":"Human Agent Controls and Input Customization"},{"location":"human_input.html#human-agent-overview","text":"The HumanAgent class provides an interface for human players to participate in matches through keyboard inputs. It translates key presses into game actions compatible with the Stable-Retro environment.","title":"Human Agent Overview"},{"location":"human_input.html#default-controls","text":"","title":"Default Controls"},{"location":"human_input.html#player-1-controls","text":"Action Default Key Movement UP \u2191 (Up Arrow) DOWN \u2193 (Down Arrow) LEFT \u2190 (Left Arrow) RIGHT \u2192 (Right Arrow) Attacks A Z B X C C START Enter MODE Tab","title":"Player 1 Controls"},{"location":"human_input.html#player-2-controls","text":"Action Default Key Movement UP W DOWN S LEFT A RIGHT D Attacks A T B Y C U START Enter MODE Tab","title":"Player 2 Controls"},{"location":"human_input.html#input-processing-pipeline","text":"The input system follows this flow: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Key Press \u2502 \u2502 InputHandler \u2502 \u2502 HumanAgent \u2502 \u2502 Game \u2502 \u2502 (Keyboard) \u2502\u2500\u2500\u2500\u2500>\u2502 (Key Tracking) \u2502\u2500\u2500\u2500\u2500>\u2502 (Action \u2502\u2500\u2500\u2500\u2500>\u2502 Environment \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 Conversion) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key presses are detected by pyglet's event system InputHandler maintains sets of pressed keys for each player HumanAgent converts key presses to binary action arrays Actions are passed to the game environment","title":"Input Processing Pipeline"},{"location":"human_input.html#how-input-works","text":"","title":"How Input Works"},{"location":"human_input.html#input-detection","text":"The InputHandler class listens for key events on the game window: def on_key_press(self, symbol, modifiers): # Track which player's key was pressed if symbol in P1_KEY_MAP: self.p1_pressed.add(symbol) elif symbol in P2_KEY_MAP: self.p2_pressed.add(symbol)","title":"Input Detection"},{"location":"human_input.html#key-to-action-conversion","text":"The HumanAgent converts tracked key presses to game actions: def select_action(self, obs, info) -> int: # Convert pressed keys to binary action array action_array = [0] * len(self.buttons) for key in self.pressed_keys: if key in self.key_map: button = self.key_map[key] if button in self.buttons: action_array[self.buttons.index(button)] = 1 # Find matching action ID for action_id, arr in enumerate(self.action_mapping): if arr == action_array: return action_id return 0 # Return NEUTRAL if no match","title":"Key-to-Action Conversion"},{"location":"human_input.html#customizing-key-mappings","text":"You can modify key mappings by editing the key map constants in the project: # In mk_ai/configs/__init__.py or similar file from pyglet.window import key as keycodes P1_KEY_MAP = { keycodes.UP: \"UP\", keycodes.DOWN: \"DOWN\", keycodes.LEFT: \"LEFT\", keycodes.RIGHT: \"RIGHT\", keycodes.Z: \"A\", keycodes.X: \"B\", keycodes.C: \"C\", # Add custom mappings here } P2_KEY_MAP = { keycodes.W: \"UP\", keycodes.S: \"DOWN\", keycodes.A: \"LEFT\", keycodes.D: \"RIGHT\", keycodes.T: \"A\", keycodes.Y: \"B\", keycodes.U: \"C\", # Add custom mappings here }","title":"Customizing Key Mappings"},{"location":"human_input.html#adding-new-input-methods","text":"","title":"Adding New Input Methods"},{"location":"human_input.html#gamepad-support","text":"To add gamepad support: Create a gamepad handler class that inherits from Agent : class GamepadAgent(Agent): def __init__(self, action_mapping, buttons, gamepad_id=0): self.action_mapping = action_mapping self.buttons = buttons self.gamepad_id = gamepad_id # Initialize gamepad library (e.g., using pyglet or pygame) self.gamepad = self.initialize_gamepad(gamepad_id) def initialize_gamepad(self, id): # Implementation depends on the gamepad library you're using pass def select_action(self, obs, info): # Read gamepad state button_states = self.read_gamepad_state() # Convert to action array similar to HumanAgent action_array = [0] * len(self.buttons) for button_name, is_pressed in button_states.items(): if is_pressed and button_name in self.buttons: action_array[self.buttons.index(button_name)] = 1 # Find matching action ID for action_id, arr in enumerate(self.action_mapping): if arr == action_array: return action_id return 0 Update AgentFactory to support the new agent type: elif config.agent_type == \"gamepad\": return GamepadAgent( action_mapping=action_mapping, buttons=buttons, gamepad_id=config.gamepad_id )","title":"Gamepad Support"},{"location":"human_input.html#network-input-online-play","text":"For networked multiplayer: Create a NetworkAgent class that receives inputs from a remote player Implement a simple socket server to relay player inputs Update the AgentFactory to support the new agent type","title":"Network Input (Online Play)"},{"location":"human_input.html#troubleshooting-input-issues","text":"","title":"Troubleshooting Input Issues"},{"location":"human_input.html#common-issues-and-solutions","text":"Keys not responding: Check that the key mapping exists in P1_KEY_MAP or P2_KEY_MAP Verify the window has focus when keys are pressed Multiple keys not working together: Some keyboards have limited key rollover. Try pressing fewer keys simultaneously Check if the key combination exists in env_config.yaml Key stuck on: Ensure on_key_release is being called properly Add manual cleanup code to reset keys when losing window focus","title":"Common Issues and Solutions"},{"location":"human_input.html#example-adding-wasd-controls-for-player-1","text":"from pyglet.window import key as keycodes # Modified key mapping that uses WASD for P1 movement P1_KEY_MAP = { # WASD movement keycodes.W: \"UP\", keycodes.A: \"LEFT\", keycodes.S: \"DOWN\", keycodes.D: \"RIGHT\", # Attack buttons keycodes.J: \"A\", keycodes.K: \"B\", keycodes.L: \"C\", keycodes.ENTER: \"START\", keycodes.SPACE: \"MODE\", }","title":"Example: Adding WASD Controls for Player 1"},{"location":"human_input.html#advanced-creating-custom-input-handlers","text":"For specialized input devices or complex control schemes, you can create custom input handlers: Create a new class that inherits from InputHandler Override the key event methods to support your devices Update your custom HumanAgent to work with your handler class CustomInputHandler(InputHandler): def __init__(self, window): super().__init__(window) # Initialize your custom input devices def setup_window_events(self): super().setup_window_events() # Add custom event handlers def update(self): # Poll custom input devices and update key states pass Then modify your game loop to use this custom handler. Similar code found with 1 license type","title":"Advanced: Creating Custom Input Handlers"},{"location":"model_evaluation.html","text":"Evaluating RL Models for Mortal Kombat II This guide explains how to thoroughly evaluate trained reinforcement learning models using the Kane vs Abel framework. Why Evaluate? Proper evaluation helps: - Determine if a model is ready for deployment - Compare different training approaches - Identify specific weaknesses or failure modes - Track improvements during fine-tuning - Understand generalization to new opponents/scenarios Using the Evaluation Script The Kane vs Abel framework provides a comprehensive evaluation script ( test.py ) with multiple evaluation modes. Basic Usage python test.py --model_path models/my_model.zip --model_type DUELINGDDQN --num_episodes 10 Command Line Options Option Description --model_path Path to the saved model file --model_type Model type (DQN, DDQN, DUELINGDDQN, PPO) --game Name of the ROM/game (default: MortalKombatII-Genesis) --state Game state to load for evaluation --states Comma-separated list of states to evaluate on --individual_eval If set, evaluate each state individually --render_mode Render mode (human, rgb_array, none) --num_stack Number of frames to stack (default: 4) --num_skip Number of frames to skip (default: 4) --num_episodes Number of episodes for evaluation (default: 10) Evaluation Modes Single State Evaluation Evaluate on a single, specific game state: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --state \"Level1.LiuKangVsJax\" --render_mode human This mode is useful for: - Visually inspecting model behavior against a specific opponent - Focused testing of challenging scenarios - Direct comparison between models on a standard benchmark Multiple States Evaluation (Combined) Evaluate across multiple states with random sampling: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --states \"VeryEasy.LiuKang-04,VeryEasy.LiuKang-05,VeryEasy.LiuKang-06\" This mode is useful for: - Testing overall model robustness - Getting an aggregate performance metric - Simulating varied opponent encounters Multiple States Evaluation (Individual) Evaluate each state separately with individual results: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --states \"VeryEasy.LiuKang-04,VeryEasy.LiuKang-05\" --individual_eval This mode is useful for: - Identifying specific strengths/weaknesses against different opponents - Granular performance analysis - Detecting overfitting to specific scenarios Understanding Evaluation Results The evaluation script generates CSV files with detailed metrics: Output Format episode,reward,won 1,253.0,True 2,187.5,True 3,-52.5,False ... average,178.2, std,97.5, Result Analysis Key metrics to examine: Average Reward : Higher is better, but context matters: 200+: Excellent performance (usually wins consistently) 100-200: Good performance (wins most matches) 0-100: Mediocre performance (inconsistent results) <0: Poor performance (usually loses) Win Rate : Percentage of episodes won Primary indicator of agent effectiveness Consider alongside average reward (some wins might be barely scraped) Standard Deviation : Indicates consistency: Low std dev + high average: Consistent strong performance High std dev: Inconsistent (sometimes great, sometimes terrible) Per-State Performance : For individual evaluations Identifies matchup-specific strengths/weaknesses Helps target fine-tuning efforts Evaluation Strategies Visualization Use the --render_mode human option to visually observe agent behavior: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --state \"Level1.LiuKangVsJax\" --render_mode human --num_episodes 3 This helps identify: - Action patterns and strategies - Positioning and spacing behavior - Defensive reactions and counter-attacks - Obvious mistakes or sub-optimal behaviors Cross-Model Comparison To compare multiple models: Evaluate each model on the same set of states: bash python test.py --model_path models/model_A.zip --model_type DUELINGDDQN --states \"state1,state2,state3\" --individual_eval python test.py --model_path models/model_B.zip --model_type DDQN --states \"state1,state2,state3\" --individual_eval Compare results using the generated CSV files Consider implementing an automated comparison script for large-scale evaluations Stress Testing Test resilience by evaluating on particularly challenging scenarios: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --states \"Hard.LiuKang-01,VeryHard.LiuKang-02\" --individual_eval Ensemble Evaluation For critical applications, use ensemble evaluation across many episodes (30+) for statistical significance: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --states \"State1,State2,State3,State4,State5\" --individual_eval --num_episodes 30 Customizing the Evaluation Framework Custom Metrics You can extend the evaluate_agent function in test.py to track additional metrics: def evaluate_agent(model, env, num_episodes=10): # ...existing code... additional_metrics = { 'combos_performed': [], 'avg_reaction_time': [], 'defensive_actions': [] } # ...track these during evaluation loop... return avg_reward, std_reward, episode_rewards, episode_wins, additional_metrics Best Practices for Evaluation Statistical Significance : Always evaluate on enough episodes (10 minimum, 30+ preferred) Diverse Scenarios : Test on states both seen and unseen during training Controlled Comparisons : Use identical seeds when comparing models Regular Benchmarking : Establish standard evaluation scenarios for ongoing development Version Control : Track evaluation results alongside model versions Reproducibility : Document exact evaluation parameters Reality Check : Supplement metrics with visual inspection Troubleshooting Evaluation Common issues and solutions: Inconsistent Results : Increase number of evaluation episodes Model Loading Errors : Verify model type matches what's specified Environment Errors : Check that specified game states exist in your ROM Performance Gaps : Compare against baseline models or human performance Resource Usage : For large evaluations, reduce render quality or use no rendering","title":"Model Evaluation"},{"location":"model_evaluation.html#evaluating-rl-models-for-mortal-kombat-ii","text":"This guide explains how to thoroughly evaluate trained reinforcement learning models using the Kane vs Abel framework.","title":"Evaluating RL Models for Mortal Kombat II"},{"location":"model_evaluation.html#why-evaluate","text":"Proper evaluation helps: - Determine if a model is ready for deployment - Compare different training approaches - Identify specific weaknesses or failure modes - Track improvements during fine-tuning - Understand generalization to new opponents/scenarios","title":"Why Evaluate?"},{"location":"model_evaluation.html#using-the-evaluation-script","text":"The Kane vs Abel framework provides a comprehensive evaluation script ( test.py ) with multiple evaluation modes.","title":"Using the Evaluation Script"},{"location":"model_evaluation.html#basic-usage","text":"python test.py --model_path models/my_model.zip --model_type DUELINGDDQN --num_episodes 10","title":"Basic Usage"},{"location":"model_evaluation.html#command-line-options","text":"Option Description --model_path Path to the saved model file --model_type Model type (DQN, DDQN, DUELINGDDQN, PPO) --game Name of the ROM/game (default: MortalKombatII-Genesis) --state Game state to load for evaluation --states Comma-separated list of states to evaluate on --individual_eval If set, evaluate each state individually --render_mode Render mode (human, rgb_array, none) --num_stack Number of frames to stack (default: 4) --num_skip Number of frames to skip (default: 4) --num_episodes Number of episodes for evaluation (default: 10)","title":"Command Line Options"},{"location":"model_evaluation.html#evaluation-modes","text":"","title":"Evaluation Modes"},{"location":"model_evaluation.html#single-state-evaluation","text":"Evaluate on a single, specific game state: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --state \"Level1.LiuKangVsJax\" --render_mode human This mode is useful for: - Visually inspecting model behavior against a specific opponent - Focused testing of challenging scenarios - Direct comparison between models on a standard benchmark","title":"Single State Evaluation"},{"location":"model_evaluation.html#multiple-states-evaluation-combined","text":"Evaluate across multiple states with random sampling: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --states \"VeryEasy.LiuKang-04,VeryEasy.LiuKang-05,VeryEasy.LiuKang-06\" This mode is useful for: - Testing overall model robustness - Getting an aggregate performance metric - Simulating varied opponent encounters","title":"Multiple States Evaluation (Combined)"},{"location":"model_evaluation.html#multiple-states-evaluation-individual","text":"Evaluate each state separately with individual results: python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \\ --states \"VeryEasy.LiuKang-04,VeryEasy.LiuKang-05\" --individual_eval This mode is useful for: - Identifying specific strengths/weaknesses against different opponents - Granular performance analysis - Detecting overfitting to specific scenarios","title":"Multiple States Evaluation (Individual)"},{"location":"model_evaluation.html#understanding-evaluation-results","text":"The evaluation script generates CSV files with detailed metrics:","title":"Understanding Evaluation Results"},{"location":"model_evaluation.html#output-format","text":"episode,reward,won 1,253.0,True 2,187.5,True 3,-52.5,False ... average,178.2, std,97.5,","title":"Output Format"},{"location":"model_evaluation.html#result-analysis","text":"Key metrics to examine: Average Reward : Higher is better, but context matters: 200+: Excellent performance (usually wins consistently) 100-200: Good performance (wins most matches) 0-100: Mediocre performance (inconsistent results) <0: Poor performance (usually loses) Win Rate : Percentage of episodes won Primary indicator of agent effectiveness Consider alongside average reward (some wins might be barely scraped) Standard Deviation : Indicates consistency: Low std dev + high average: Consistent strong performance High std dev: Inconsistent (sometimes great, sometimes terrible) Per-State Performance : For individual evaluations Identifies matchup-specific strengths/weaknesses Helps target fine-tuning efforts","title":"Result Analysis"},{"location":"model_evaluation.html#evaluation-strategies","text":"","title":"Evaluation Strategies"},{"location":"model_evaluation.html#visualization","text":"Use the --render_mode human option to visually observe agent behavior: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --state \"Level1.LiuKangVsJax\" --render_mode human --num_episodes 3 This helps identify: - Action patterns and strategies - Positioning and spacing behavior - Defensive reactions and counter-attacks - Obvious mistakes or sub-optimal behaviors","title":"Visualization"},{"location":"model_evaluation.html#cross-model-comparison","text":"To compare multiple models: Evaluate each model on the same set of states: bash python test.py --model_path models/model_A.zip --model_type DUELINGDDQN --states \"state1,state2,state3\" --individual_eval python test.py --model_path models/model_B.zip --model_type DDQN --states \"state1,state2,state3\" --individual_eval Compare results using the generated CSV files Consider implementing an automated comparison script for large-scale evaluations","title":"Cross-Model Comparison"},{"location":"model_evaluation.html#stress-testing","text":"Test resilience by evaluating on particularly challenging scenarios: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --states \"Hard.LiuKang-01,VeryHard.LiuKang-02\" --individual_eval","title":"Stress Testing"},{"location":"model_evaluation.html#ensemble-evaluation","text":"For critical applications, use ensemble evaluation across many episodes (30+) for statistical significance: python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \\ --states \"State1,State2,State3,State4,State5\" --individual_eval --num_episodes 30","title":"Ensemble Evaluation"},{"location":"model_evaluation.html#customizing-the-evaluation-framework","text":"","title":"Customizing the Evaluation Framework"},{"location":"model_evaluation.html#custom-metrics","text":"You can extend the evaluate_agent function in test.py to track additional metrics: def evaluate_agent(model, env, num_episodes=10): # ...existing code... additional_metrics = { 'combos_performed': [], 'avg_reaction_time': [], 'defensive_actions': [] } # ...track these during evaluation loop... return avg_reward, std_reward, episode_rewards, episode_wins, additional_metrics","title":"Custom Metrics"},{"location":"model_evaluation.html#best-practices-for-evaluation","text":"Statistical Significance : Always evaluate on enough episodes (10 minimum, 30+ preferred) Diverse Scenarios : Test on states both seen and unseen during training Controlled Comparisons : Use identical seeds when comparing models Regular Benchmarking : Establish standard evaluation scenarios for ongoing development Version Control : Track evaluation results alongside model versions Reproducibility : Document exact evaluation parameters Reality Check : Supplement metrics with visual inspection","title":"Best Practices for Evaluation"},{"location":"model_evaluation.html#troubleshooting-evaluation","text":"Common issues and solutions: Inconsistent Results : Increase number of evaluation episodes Model Loading Errors : Verify model type matches what's specified Environment Errors : Check that specified game states exist in your ROM Performance Gaps : Compare against baseline models or human performance Resource Usage : For large evaluations, reduce render quality or use no rendering","title":"Troubleshooting Evaluation"},{"location":"model_finetuning.html","text":"Fine-tuning RL Models for Mortal Kombat II This guide explains how to fine-tune pre-trained reinforcement learning models to improve their performance in specific scenarios or against challenging opponents. What is Fine-tuning? Fine-tuning is the process of continuing training on a pre-trained model, focusing on specific areas that need improvement. It's a form of transfer learning that leverages knowledge gained from prior training to efficiently adapt to new challenges. When to Fine-tune Consider fine-tuning your models when: They perform well overall but struggle against specific opponents You want to improve performance in advanced scenarios without starting from scratch You need to adapt a general model to a specialized task You've reached a performance plateau with regular training You have limited computational resources for full retraining Using the Fine-tuning Script The Kane vs Abel framework includes a dedicated fine-tuning script ( finetune.py ) that handles loading pre-trained models and continuing their training on challenging scenarios. Basic Usage python finetune.py By default, this will: 1. Load the model specified in the script 2. Create environments with challenging states 3. Continue training for a defined number of timesteps 4. Save the fine-tuned model Fine-tuning Process 1. Select a Pre-trained Model Choose a well-performing base model to fine-tune: pretrained_model_path = os.path.join(MODEL_DIR, \"kane\", \"DuellingDDQN_curriculum_16M_VeryEasy_3_Tiers\") model = DuelingDoubleDQN.load(pretrained_model_path, env=env, device=\"cuda\") 2. Define Challenging States Select specific challenging states where the model needs improvement: challenging_states = [ \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\", \"VeryEasy.LiuKang-06\", \"VeryEasy.LiuKang-07\", \"VeryEasy.LiuKang-08\" ] 3. Adjust Learning Rate for Fine-tuning Use a more conservative learning rate to avoid catastrophic forgetting: # Lower starting rate and gentler decay for fine-tuning fine_tune_lr = Schedules.linear_decay(1e-4, 1e-5) # Original might be 3e-4 to 1e-5 model.learning_rate = fine_tune_lr 4. Optionally Freeze Early Layers To preserve learned features while allowing adaptation in higher layers: # Uncomment to freeze feature extractor # for param in model.policy.features_extractor.parameters(): # param.requires_grad = False 5. Configure Evaluation During Fine-tuning Set up evaluation callbacks to monitor progress: eval_callback = CustomEvalCallback( eval_env=eval_env, best_model_save_path=os.path.join(LOG_DIR, \"fine_tuned_best\"), log_path=os.path.join(LOG_DIR, \"fine_tune_eval\"), eval_freq=62_500, # How often to evaluate n_eval_episodes=10, deterministic=False, render=False, verbose=1 ) 6. Continue Training Train for additional timesteps, typically fewer than the original training: model.learn( total_timesteps=2_000_000, # Less than original training reset_num_timesteps=False, # Continue from previous steps callback=eval_callback, ) Customizing Fine-tuning Target Specific Weaknesses Identify and focus on specific weaknesses through careful state selection: # Example: Focus on specific opponents challenging_states = [ \"Medium.LiuKangVsBaraka\", # If struggling against Baraka \"Hard.LiuKangVsReptile\" # If struggling against Reptile ] Learning Rate Schedule Strategies Different schedules for different fine-tuning goals: # For minor adjustments (conservative) conservative_lr = Schedules.linear_decay(5e-5, 1e-6) # For significant adaptation (more aggressive) adaptive_lr = Schedules.linear_decay(1e-4, 1e-5) # For focused, short fine-tuning cyclical_lr = Schedules.cyclical_lr(5e-5, 1e-4, 0.5) Selective Layer Freezing For more control over what parts of the model adapt: # Option 1: Freeze just the convolutional base for name, param in model.policy.features_extractor.cnn.named_parameters(): param.requires_grad = False # Option 2: Freeze specific layers for name, param in model.policy.named_parameters(): if \"features_extractor\" in name: param.requires_grad = False if \"q_net.0\" in name: # First layer after feature extraction param.requires_grad = False Exploration Settings Adjust exploration parameters for fine-tuning: # Reduce exploration for fine-tuning model.exploration_schedule = Schedules.linear_decay(0.1, 0.01) # Lower than original Advanced Fine-tuning Techniques Regularization for Fine-tuning Add regularization to prevent overfitting to the new states: # Add L2 regularization to optimizer (example for Adam) from torch import optim # Get current params optimizer_params = model.policy.optimizer.defaults # Add weight decay (L2 regularization) optimizer_params['weight_decay'] = 1e-4 # Recreate optimizer model.policy.optimizer = optim.Adam( model.policy.parameters(), **optimizer_params ) Elastic Weight Consolidation For complex models, implement Elastic Weight Consolidation (EWC) to preserve important weights: # Pseudo-code for EWC implementation original_params = {name: param.clone().detach() for name, param in model.named_parameters()} fisher_information = estimate_fisher_information(model, old_env) def ewc_loss(model, original_params, fisher_information, lambda_ewc=5000): loss = 0 for name, param in model.named_parameters(): loss += fisher_information[name] * (param - original_params[name]).pow(2).sum() return lambda_ewc * loss Rehearsal with Mixed Experiences Fine-tune with a mix of old and new experiences to prevent forgetting: # Create environments with mixed states mixed_states = original_training_states + challenging_states mixed_env = SubprocVecEnv([make_env(mixed_states) for _ in range(NUM_ENVS)]) Progressive Fine-tuning Gradually introduce challenging scenarios: # Start with a mix favoring original states phase1_states = original_states + [challenging_states[0]] # Then introduce more challenging states phase2_states = original_states + challenging_states[:3] # Finally use all challenging states phase3_states = challenging_states Evaluating Fine-tuning Success After fine-tuning, evaluate comprehensively: # Evaluate on original states python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states \"original_state1,original_state2\" --individual_eval python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states \"original_state1,original_state2\" --individual_eval # Evaluate on challenging states python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states \"challenging_state1,challenging_state2\" --individual_eval python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states \"challenging_state1,challenging_state2\" --individual_eval Look for: 1. Improvement on challenging states 2. Maintenance of performance on original states 3. Overall win rate changes 4. Changes in behavior patterns Example: Complete Fine-tuning Workflow import os from mk_ai.agents import DuelingDoubleDQN from mk_ai.utils import Schedules from mk_ai.callbacks import CustomEvalCallback from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack # 1. Identify where the model is struggling (through evaluation) # python test.py --model_path models/base_model.zip --model_type DUELINGDDQN --states \"all_states\" --individual_eval # 2. Select challenging states based on evaluation challenging_states = [ \"VeryEasy.LiuKang-06\", # Low win rate identified here \"VeryEasy.LiuKang-07\", # Low win rate identified here \"VeryEasy.LiuKang-08\" # Low win rate identified here ] # 3. Create environments for fine-tuning env = SubprocVecEnv([make_env(challenging_states) for _ in range(8)]) env = VecFrameStack(env, n_stack=4) # 4. Load pre-trained model model = DuelingDoubleDQN.load(\"models/base_model.zip\", env=env, device=\"cuda\") # 5. Configure for fine-tuning model.learning_rate = Schedules.linear_decay(1e-4, 1e-5) # 6. Setup evaluation eval_env = DummyVecEnv([make_env(challenging_states)]) eval_env = VecFrameStack(eval_env, n_stack=4) eval_callback = CustomEvalCallback( eval_env=eval_env, best_model_save_path=\"./logs/fine_tuned_best\", log_path=\"./logs/fine_tune_eval\", eval_freq=60000, n_eval_episodes=10 ) # 7. Fine-tune the model model.learn( total_timesteps=2_000_000, reset_num_timesteps=False, callback=eval_callback ) # 8. Save fine-tuned model model.save(\"models/fine_tuned_model\") # 9. Evaluate the fine-tuned model # python test.py --model_path models/fine_tuned_model.zip --model_type DUELINGDDQN --states \"all_states\" --individual_eval Best Practices for Fine-tuning Always evaluate before and after to quantify improvements Start with a lower learning rate than the original training Be selective with states - target specific weaknesses Monitor for catastrophic forgetting on original scenarios Keep fine-tuning sessions shorter than original training Save intermediate checkpoints during fine-tuning Consider layer freezing for specialized adaptations Use TensorBoard to monitor the fine-tuning process","title":"Fine-tuning Models"},{"location":"model_finetuning.html#fine-tuning-rl-models-for-mortal-kombat-ii","text":"This guide explains how to fine-tune pre-trained reinforcement learning models to improve their performance in specific scenarios or against challenging opponents.","title":"Fine-tuning RL Models for Mortal Kombat II"},{"location":"model_finetuning.html#what-is-fine-tuning","text":"Fine-tuning is the process of continuing training on a pre-trained model, focusing on specific areas that need improvement. It's a form of transfer learning that leverages knowledge gained from prior training to efficiently adapt to new challenges.","title":"What is Fine-tuning?"},{"location":"model_finetuning.html#when-to-fine-tune","text":"Consider fine-tuning your models when: They perform well overall but struggle against specific opponents You want to improve performance in advanced scenarios without starting from scratch You need to adapt a general model to a specialized task You've reached a performance plateau with regular training You have limited computational resources for full retraining","title":"When to Fine-tune"},{"location":"model_finetuning.html#using-the-fine-tuning-script","text":"The Kane vs Abel framework includes a dedicated fine-tuning script ( finetune.py ) that handles loading pre-trained models and continuing their training on challenging scenarios.","title":"Using the Fine-tuning Script"},{"location":"model_finetuning.html#basic-usage","text":"python finetune.py By default, this will: 1. Load the model specified in the script 2. Create environments with challenging states 3. Continue training for a defined number of timesteps 4. Save the fine-tuned model","title":"Basic Usage"},{"location":"model_finetuning.html#fine-tuning-process","text":"","title":"Fine-tuning Process"},{"location":"model_finetuning.html#1-select-a-pre-trained-model","text":"Choose a well-performing base model to fine-tune: pretrained_model_path = os.path.join(MODEL_DIR, \"kane\", \"DuellingDDQN_curriculum_16M_VeryEasy_3_Tiers\") model = DuelingDoubleDQN.load(pretrained_model_path, env=env, device=\"cuda\")","title":"1. Select a Pre-trained Model"},{"location":"model_finetuning.html#2-define-challenging-states","text":"Select specific challenging states where the model needs improvement: challenging_states = [ \"VeryEasy.LiuKang-04\", \"VeryEasy.LiuKang-05\", \"VeryEasy.LiuKang-06\", \"VeryEasy.LiuKang-07\", \"VeryEasy.LiuKang-08\" ]","title":"2. Define Challenging States"},{"location":"model_finetuning.html#3-adjust-learning-rate-for-fine-tuning","text":"Use a more conservative learning rate to avoid catastrophic forgetting: # Lower starting rate and gentler decay for fine-tuning fine_tune_lr = Schedules.linear_decay(1e-4, 1e-5) # Original might be 3e-4 to 1e-5 model.learning_rate = fine_tune_lr","title":"3. Adjust Learning Rate for Fine-tuning"},{"location":"model_finetuning.html#4-optionally-freeze-early-layers","text":"To preserve learned features while allowing adaptation in higher layers: # Uncomment to freeze feature extractor # for param in model.policy.features_extractor.parameters(): # param.requires_grad = False","title":"4. Optionally Freeze Early Layers"},{"location":"model_finetuning.html#5-configure-evaluation-during-fine-tuning","text":"Set up evaluation callbacks to monitor progress: eval_callback = CustomEvalCallback( eval_env=eval_env, best_model_save_path=os.path.join(LOG_DIR, \"fine_tuned_best\"), log_path=os.path.join(LOG_DIR, \"fine_tune_eval\"), eval_freq=62_500, # How often to evaluate n_eval_episodes=10, deterministic=False, render=False, verbose=1 )","title":"5. Configure Evaluation During Fine-tuning"},{"location":"model_finetuning.html#6-continue-training","text":"Train for additional timesteps, typically fewer than the original training: model.learn( total_timesteps=2_000_000, # Less than original training reset_num_timesteps=False, # Continue from previous steps callback=eval_callback, )","title":"6. Continue Training"},{"location":"model_finetuning.html#customizing-fine-tuning","text":"","title":"Customizing Fine-tuning"},{"location":"model_finetuning.html#target-specific-weaknesses","text":"Identify and focus on specific weaknesses through careful state selection: # Example: Focus on specific opponents challenging_states = [ \"Medium.LiuKangVsBaraka\", # If struggling against Baraka \"Hard.LiuKangVsReptile\" # If struggling against Reptile ]","title":"Target Specific Weaknesses"},{"location":"model_finetuning.html#learning-rate-schedule-strategies","text":"Different schedules for different fine-tuning goals: # For minor adjustments (conservative) conservative_lr = Schedules.linear_decay(5e-5, 1e-6) # For significant adaptation (more aggressive) adaptive_lr = Schedules.linear_decay(1e-4, 1e-5) # For focused, short fine-tuning cyclical_lr = Schedules.cyclical_lr(5e-5, 1e-4, 0.5)","title":"Learning Rate Schedule Strategies"},{"location":"model_finetuning.html#selective-layer-freezing","text":"For more control over what parts of the model adapt: # Option 1: Freeze just the convolutional base for name, param in model.policy.features_extractor.cnn.named_parameters(): param.requires_grad = False # Option 2: Freeze specific layers for name, param in model.policy.named_parameters(): if \"features_extractor\" in name: param.requires_grad = False if \"q_net.0\" in name: # First layer after feature extraction param.requires_grad = False","title":"Selective Layer Freezing"},{"location":"model_finetuning.html#exploration-settings","text":"Adjust exploration parameters for fine-tuning: # Reduce exploration for fine-tuning model.exploration_schedule = Schedules.linear_decay(0.1, 0.01) # Lower than original","title":"Exploration Settings"},{"location":"model_finetuning.html#advanced-fine-tuning-techniques","text":"","title":"Advanced Fine-tuning Techniques"},{"location":"model_finetuning.html#regularization-for-fine-tuning","text":"Add regularization to prevent overfitting to the new states: # Add L2 regularization to optimizer (example for Adam) from torch import optim # Get current params optimizer_params = model.policy.optimizer.defaults # Add weight decay (L2 regularization) optimizer_params['weight_decay'] = 1e-4 # Recreate optimizer model.policy.optimizer = optim.Adam( model.policy.parameters(), **optimizer_params )","title":"Regularization for Fine-tuning"},{"location":"model_finetuning.html#elastic-weight-consolidation","text":"For complex models, implement Elastic Weight Consolidation (EWC) to preserve important weights: # Pseudo-code for EWC implementation original_params = {name: param.clone().detach() for name, param in model.named_parameters()} fisher_information = estimate_fisher_information(model, old_env) def ewc_loss(model, original_params, fisher_information, lambda_ewc=5000): loss = 0 for name, param in model.named_parameters(): loss += fisher_information[name] * (param - original_params[name]).pow(2).sum() return lambda_ewc * loss","title":"Elastic Weight Consolidation"},{"location":"model_finetuning.html#rehearsal-with-mixed-experiences","text":"Fine-tune with a mix of old and new experiences to prevent forgetting: # Create environments with mixed states mixed_states = original_training_states + challenging_states mixed_env = SubprocVecEnv([make_env(mixed_states) for _ in range(NUM_ENVS)])","title":"Rehearsal with Mixed Experiences"},{"location":"model_finetuning.html#progressive-fine-tuning","text":"Gradually introduce challenging scenarios: # Start with a mix favoring original states phase1_states = original_states + [challenging_states[0]] # Then introduce more challenging states phase2_states = original_states + challenging_states[:3] # Finally use all challenging states phase3_states = challenging_states","title":"Progressive Fine-tuning"},{"location":"model_finetuning.html#evaluating-fine-tuning-success","text":"After fine-tuning, evaluate comprehensively: # Evaluate on original states python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states \"original_state1,original_state2\" --individual_eval python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states \"original_state1,original_state2\" --individual_eval # Evaluate on challenging states python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states \"challenging_state1,challenging_state2\" --individual_eval python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states \"challenging_state1,challenging_state2\" --individual_eval Look for: 1. Improvement on challenging states 2. Maintenance of performance on original states 3. Overall win rate changes 4. Changes in behavior patterns","title":"Evaluating Fine-tuning Success"},{"location":"model_finetuning.html#example-complete-fine-tuning-workflow","text":"import os from mk_ai.agents import DuelingDoubleDQN from mk_ai.utils import Schedules from mk_ai.callbacks import CustomEvalCallback from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack # 1. Identify where the model is struggling (through evaluation) # python test.py --model_path models/base_model.zip --model_type DUELINGDDQN --states \"all_states\" --individual_eval # 2. Select challenging states based on evaluation challenging_states = [ \"VeryEasy.LiuKang-06\", # Low win rate identified here \"VeryEasy.LiuKang-07\", # Low win rate identified here \"VeryEasy.LiuKang-08\" # Low win rate identified here ] # 3. Create environments for fine-tuning env = SubprocVecEnv([make_env(challenging_states) for _ in range(8)]) env = VecFrameStack(env, n_stack=4) # 4. Load pre-trained model model = DuelingDoubleDQN.load(\"models/base_model.zip\", env=env, device=\"cuda\") # 5. Configure for fine-tuning model.learning_rate = Schedules.linear_decay(1e-4, 1e-5) # 6. Setup evaluation eval_env = DummyVecEnv([make_env(challenging_states)]) eval_env = VecFrameStack(eval_env, n_stack=4) eval_callback = CustomEvalCallback( eval_env=eval_env, best_model_save_path=\"./logs/fine_tuned_best\", log_path=\"./logs/fine_tune_eval\", eval_freq=60000, n_eval_episodes=10 ) # 7. Fine-tune the model model.learn( total_timesteps=2_000_000, reset_num_timesteps=False, callback=eval_callback ) # 8. Save fine-tuned model model.save(\"models/fine_tuned_model\") # 9. Evaluate the fine-tuned model # python test.py --model_path models/fine_tuned_model.zip --model_type DUELINGDDQN --states \"all_states\" --individual_eval","title":"Example: Complete Fine-tuning Workflow"},{"location":"model_finetuning.html#best-practices-for-fine-tuning","text":"Always evaluate before and after to quantify improvements Start with a lower learning rate than the original training Be selective with states - target specific weaknesses Monitor for catastrophic forgetting on original scenarios Keep fine-tuning sessions shorter than original training Save intermediate checkpoints during fine-tuning Consider layer freezing for specialized adaptations Use TensorBoard to monitor the fine-tuning process","title":"Best Practices for Fine-tuning"},{"location":"model_training.html","text":"Training Custom RL Models for Mortal Kombat II This guide provides an overview of training reinforcement learning models for the Mortal Kombat II environment. For more detailed information on specific topics, see the dedicated guides: Curriculum Learning : Progressive training through increasing difficulty levels Model Evaluation : Testing and analyzing model performance Fine-tuning Models : Improving pre-trained models for specific scenarios Training a Model from Scratch The train.py script provides a comprehensive framework for training RL models with various advanced features including optional curriculum learning and learning rate schedules. Basic Usage python train.py Training Process Overview Environment Setup : The script creates vectorized environments using SubprocVecEnv for parallel training Model Initialization : Models are configured with hyperparameters optimized for fighting games Training Loop : The model learns for a defined number of timesteps with periodic evaluation Model Saving : The final model and best models during training are saved for later use Customizing Training To customize your training, edit train.py and modify: Model Type Choose from: - DQN : Standard Deep Q-Network - DoubleDQN : Double DQN for more stable training - DuelingDoubleDQN : Dueling architecture with Double DQN updates # Example: Change model type model = DuelingDoubleDQN( # or DoubleDQN or DQN env=stacked_env, verbose=1, device=\"cuda\", # other parameters... ) Learning Rate Schedules Choose from multiple learning rate schedules in mk_ai.utils.schedulers.Schedules : # Linear decay lr_schedule = Schedules.linear_decay(3.16e-4, 1e-5) # Exponential decay exp_decay_lr = Schedules.exponential_decay(3.16e-4, 0.295) # Cyclical learning rates cyclical_lr = Schedules.cyclical_lr(1e-4, 3e-4, 0.5) Hyperparameters Tune key hyperparameters for your specific training needs: model = DuelingDoubleDQN( env=stacked_env, buffer_size=200000, # Replay buffer size batch_size=32, # Batch size for updates gamma=0.95, # Discount factor learning_rate=lr_schedule, exploration_fraction=0.3, # Fraction of training for exploration exploration_initial_eps=0.9, exploration_final_eps=0.07, tensorboard_log=\"./logs/my_custom_model/\" ) Monitoring Training Progress View training progress with TensorBoard: tensorboard --logdir=./experiments_finals This will show: - Learning curves (reward, loss) - Exploration rate decay - Evaluation performance Best Practices Training Start with a simpler environment and gradually increase difficulty Use curriculum learning for complex environments (see Curriculum Learning ) Monitor training statistics via TensorBoard logs Save regular checkpoints during long training sessions Consider using parallel environments (SubprocVecEnv) to speed up training Experiment with different model architectures and learning rate schedules Troubleshooting Common Issues GPU Out of Memory : Reduce batch size or number of parallel environments Slow Learning : Adjust learning rate schedule or exploration parameters Overfitting : Increase environment variety or add regularization Debugging Tips Add verbose logging to track specific behaviors Use render_mode=\"human\" for visual debugging of agent behaviors Implement custom callbacks for detailed monitoring","title":"Overview"},{"location":"model_training.html#training-custom-rl-models-for-mortal-kombat-ii","text":"This guide provides an overview of training reinforcement learning models for the Mortal Kombat II environment. For more detailed information on specific topics, see the dedicated guides: Curriculum Learning : Progressive training through increasing difficulty levels Model Evaluation : Testing and analyzing model performance Fine-tuning Models : Improving pre-trained models for specific scenarios","title":"Training Custom RL Models for Mortal Kombat II"},{"location":"model_training.html#training-a-model-from-scratch","text":"The train.py script provides a comprehensive framework for training RL models with various advanced features including optional curriculum learning and learning rate schedules.","title":"Training a Model from Scratch"},{"location":"model_training.html#basic-usage","text":"python train.py","title":"Basic Usage"},{"location":"model_training.html#training-process-overview","text":"Environment Setup : The script creates vectorized environments using SubprocVecEnv for parallel training Model Initialization : Models are configured with hyperparameters optimized for fighting games Training Loop : The model learns for a defined number of timesteps with periodic evaluation Model Saving : The final model and best models during training are saved for later use","title":"Training Process Overview"},{"location":"model_training.html#customizing-training","text":"To customize your training, edit train.py and modify:","title":"Customizing Training"},{"location":"model_training.html#model-type","text":"Choose from: - DQN : Standard Deep Q-Network - DoubleDQN : Double DQN for more stable training - DuelingDoubleDQN : Dueling architecture with Double DQN updates # Example: Change model type model = DuelingDoubleDQN( # or DoubleDQN or DQN env=stacked_env, verbose=1, device=\"cuda\", # other parameters... )","title":"Model Type"},{"location":"model_training.html#learning-rate-schedules","text":"Choose from multiple learning rate schedules in mk_ai.utils.schedulers.Schedules : # Linear decay lr_schedule = Schedules.linear_decay(3.16e-4, 1e-5) # Exponential decay exp_decay_lr = Schedules.exponential_decay(3.16e-4, 0.295) # Cyclical learning rates cyclical_lr = Schedules.cyclical_lr(1e-4, 3e-4, 0.5)","title":"Learning Rate Schedules"},{"location":"model_training.html#hyperparameters","text":"Tune key hyperparameters for your specific training needs: model = DuelingDoubleDQN( env=stacked_env, buffer_size=200000, # Replay buffer size batch_size=32, # Batch size for updates gamma=0.95, # Discount factor learning_rate=lr_schedule, exploration_fraction=0.3, # Fraction of training for exploration exploration_initial_eps=0.9, exploration_final_eps=0.07, tensorboard_log=\"./logs/my_custom_model/\" )","title":"Hyperparameters"},{"location":"model_training.html#monitoring-training-progress","text":"View training progress with TensorBoard: tensorboard --logdir=./experiments_finals This will show: - Learning curves (reward, loss) - Exploration rate decay - Evaluation performance","title":"Monitoring Training Progress"},{"location":"model_training.html#best-practices","text":"","title":"Best Practices"},{"location":"model_training.html#training","text":"Start with a simpler environment and gradually increase difficulty Use curriculum learning for complex environments (see Curriculum Learning ) Monitor training statistics via TensorBoard logs Save regular checkpoints during long training sessions Consider using parallel environments (SubprocVecEnv) to speed up training Experiment with different model architectures and learning rate schedules","title":"Training"},{"location":"model_training.html#troubleshooting","text":"","title":"Troubleshooting"},{"location":"model_training.html#common-issues","text":"GPU Out of Memory : Reduce batch size or number of parallel environments Slow Learning : Adjust learning rate schedule or exploration parameters Overfitting : Increase environment variety or add regularization","title":"Common Issues"},{"location":"model_training.html#debugging-tips","text":"Add verbose logging to track specific behaviors Use render_mode=\"human\" for visual debugging of agent behaviors Implement custom callbacks for detailed monitoring","title":"Debugging Tips"}]}