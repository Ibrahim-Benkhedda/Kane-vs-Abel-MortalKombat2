<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Model Evaluation - Kane vs Abel Arena</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Model Evaluation";
        var mkdocs_page_input_path = "model_evaluation.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Kane vs Abel Arena
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="arena.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Environment</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="action_space.html">Action Space</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="adding_diff_game.html">Additional Games</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Agents</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="behaviour_tree.html">Behaviour Tree</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="human_input.html">Human Input</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="dqn.html">DQN</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RL Training</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="model_training.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="curriculum_learning.html">Curriculum Learning</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Model Evaluation</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#why-evaluate">Why Evaluate?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#using-the-evaluation-script">Using the Evaluation Script</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#basic-usage">Basic Usage</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#command-line-options">Command Line Options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#evaluation-modes">Evaluation Modes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#single-state-evaluation">Single State Evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multiple-states-evaluation-combined">Multiple States Evaluation (Combined)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multiple-states-evaluation-individual">Multiple States Evaluation (Individual)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#understanding-evaluation-results">Understanding Evaluation Results</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-format">Output Format</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#result-analysis">Result Analysis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#evaluation-strategies">Evaluation Strategies</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#visualization">Visualization</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cross-model-comparison">Cross-Model Comparison</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#stress-testing">Stress Testing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ensemble-evaluation">Ensemble Evaluation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#customizing-the-evaluation-framework">Customizing the Evaluation Framework</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#custom-metrics">Custom Metrics</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#best-practices-for-evaluation">Best Practices for Evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#troubleshooting-evaluation">Troubleshooting Evaluation</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="model_finetuning.html">Fine-tuning Models</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Kane vs Abel Arena</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">RL Training</li>
      <li class="breadcrumb-item active">Model Evaluation</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="evaluating-rl-models-for-mortal-kombat-ii">Evaluating RL Models for Mortal Kombat II</h1>
<p>This guide explains how to thoroughly evaluate trained reinforcement learning models using the Kane vs Abel framework.</p>
<h2 id="why-evaluate">Why Evaluate?</h2>
<p>Proper evaluation helps:
- Determine if a model is ready for deployment
- Compare different training approaches
- Identify specific weaknesses or failure modes
- Track improvements during fine-tuning
- Understand generalization to new opponents/scenarios</p>
<h2 id="using-the-evaluation-script">Using the Evaluation Script</h2>
<p>The Kane vs Abel framework provides a comprehensive evaluation script (<code>test.py</code>) with multiple evaluation modes.</p>
<h3 id="basic-usage">Basic Usage</h3>
<pre><code class="language-bash">python test.py --model_path models/my_model.zip --model_type DUELINGDDQN --num_episodes 10
</code></pre>
<h3 id="command-line-options">Command Line Options</h3>
<table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--model_path</code></td>
<td>Path to the saved model file</td>
</tr>
<tr>
<td><code>--model_type</code></td>
<td>Model type (DQN, DDQN, DUELINGDDQN, PPO)</td>
</tr>
<tr>
<td><code>--game</code></td>
<td>Name of the ROM/game (default: MortalKombatII-Genesis)</td>
</tr>
<tr>
<td><code>--state</code></td>
<td>Game state to load for evaluation</td>
</tr>
<tr>
<td><code>--states</code></td>
<td>Comma-separated list of states to evaluate on</td>
</tr>
<tr>
<td><code>--individual_eval</code></td>
<td>If set, evaluate each state individually</td>
</tr>
<tr>
<td><code>--render_mode</code></td>
<td>Render mode (human, rgb_array, none)</td>
</tr>
<tr>
<td><code>--num_stack</code></td>
<td>Number of frames to stack (default: 4)</td>
</tr>
<tr>
<td><code>--num_skip</code></td>
<td>Number of frames to skip (default: 4)</td>
</tr>
<tr>
<td><code>--num_episodes</code></td>
<td>Number of episodes for evaluation (default: 10)</td>
</tr>
</tbody>
</table>
<h2 id="evaluation-modes">Evaluation Modes</h2>
<h3 id="single-state-evaluation">Single State Evaluation</h3>
<p>Evaluate on a single, specific game state:</p>
<pre><code class="language-bash">python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \
    --state &quot;Level1.LiuKangVsJax&quot; --render_mode human
</code></pre>
<p>This mode is useful for:
- Visually inspecting model behavior against a specific opponent
- Focused testing of challenging scenarios
- Direct comparison between models on a standard benchmark</p>
<h3 id="multiple-states-evaluation-combined">Multiple States Evaluation (Combined)</h3>
<p>Evaluate across multiple states with random sampling:</p>
<pre><code class="language-bash">python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \
    --states &quot;VeryEasy.LiuKang-04,VeryEasy.LiuKang-05,VeryEasy.LiuKang-06&quot;
</code></pre>
<p>This mode is useful for:
- Testing overall model robustness
- Getting an aggregate performance metric
- Simulating varied opponent encounters</p>
<h3 id="multiple-states-evaluation-individual">Multiple States Evaluation (Individual)</h3>
<p>Evaluate each state separately with individual results:</p>
<pre><code class="language-bash">python test.py --model_path models/kane/my_model.zip --model_type DUELINGDDQN \
    --states &quot;VeryEasy.LiuKang-04,VeryEasy.LiuKang-05&quot; --individual_eval
</code></pre>
<p>This mode is useful for:
- Identifying specific strengths/weaknesses against different opponents
- Granular performance analysis
- Detecting overfitting to specific scenarios</p>
<h2 id="understanding-evaluation-results">Understanding Evaluation Results</h2>
<p>The evaluation script generates CSV files with detailed metrics:</p>
<h3 id="output-format">Output Format</h3>
<pre><code>episode,reward,won
1,253.0,True
2,187.5,True
3,-52.5,False
...
average,178.2,
std,97.5,
</code></pre>
<h3 id="result-analysis">Result Analysis</h3>
<p>Key metrics to examine:</p>
<ol>
<li><strong>Average Reward</strong>: Higher is better, but context matters:</li>
<li>200+: Excellent performance (usually wins consistently)</li>
<li>100-200: Good performance (wins most matches)</li>
<li>0-100: Mediocre performance (inconsistent results)</li>
<li>
<p>&lt;0: Poor performance (usually loses)</p>
</li>
<li>
<p><strong>Win Rate</strong>: Percentage of episodes won</p>
</li>
<li>Primary indicator of agent effectiveness</li>
<li>
<p>Consider alongside average reward (some wins might be barely scraped)</p>
</li>
<li>
<p><strong>Standard Deviation</strong>: Indicates consistency:</p>
</li>
<li>Low std dev + high average: Consistent strong performance</li>
<li>
<p>High std dev: Inconsistent (sometimes great, sometimes terrible)</p>
</li>
<li>
<p><strong>Per-State Performance</strong>: For individual evaluations</p>
</li>
<li>Identifies matchup-specific strengths/weaknesses</li>
<li>Helps target fine-tuning efforts</li>
</ol>
<h2 id="evaluation-strategies">Evaluation Strategies</h2>
<h3 id="visualization">Visualization</h3>
<p>Use the <code>--render_mode human</code> option to visually observe agent behavior:</p>
<pre><code class="language-bash">python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \
    --state &quot;Level1.LiuKangVsJax&quot; --render_mode human --num_episodes 3
</code></pre>
<p>This helps identify:
- Action patterns and strategies
- Positioning and spacing behavior
- Defensive reactions and counter-attacks
- Obvious mistakes or sub-optimal behaviors</p>
<h3 id="cross-model-comparison">Cross-Model Comparison</h3>
<p>To compare multiple models:</p>
<ol>
<li>
<p>Evaluate each model on the same set of states:
   <code>bash
   python test.py --model_path models/model_A.zip --model_type DUELINGDDQN --states "state1,state2,state3" --individual_eval
   python test.py --model_path models/model_B.zip --model_type DDQN --states "state1,state2,state3" --individual_eval</code></p>
</li>
<li>
<p>Compare results using the generated CSV files</p>
</li>
<li>Consider implementing an automated comparison script for large-scale evaluations</li>
</ol>
<h3 id="stress-testing">Stress Testing</h3>
<p>Test resilience by evaluating on particularly challenging scenarios:</p>
<pre><code class="language-bash">python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \
    --states &quot;Hard.LiuKang-01,VeryHard.LiuKang-02&quot; --individual_eval
</code></pre>
<h3 id="ensemble-evaluation">Ensemble Evaluation</h3>
<p>For critical applications, use ensemble evaluation across many episodes (30+) for statistical significance:</p>
<pre><code class="language-bash">python test.py --model_path models/my_model.zip --model_type DUELINGDDQN \
    --states &quot;State1,State2,State3,State4,State5&quot; --individual_eval --num_episodes 30
</code></pre>
<h2 id="customizing-the-evaluation-framework">Customizing the Evaluation Framework</h2>
<h3 id="custom-metrics">Custom Metrics</h3>
<p>You can extend the <code>evaluate_agent</code> function in <code>test.py</code> to track additional metrics:</p>
<pre><code class="language-python">def evaluate_agent(model, env, num_episodes=10):
    # ...existing code...

    additional_metrics = {
        'combos_performed': [],
        'avg_reaction_time': [],
        'defensive_actions': []
    }

    # ...track these during evaluation loop...

    return avg_reward, std_reward, episode_rewards, episode_wins, additional_metrics
</code></pre>
<h2 id="best-practices-for-evaluation">Best Practices for Evaluation</h2>
<ol>
<li><strong>Statistical Significance</strong>: Always evaluate on enough episodes (10 minimum, 30+ preferred)</li>
<li><strong>Diverse Scenarios</strong>: Test on states both seen and unseen during training</li>
<li><strong>Controlled Comparisons</strong>: Use identical seeds when comparing models</li>
<li><strong>Regular Benchmarking</strong>: Establish standard evaluation scenarios for ongoing development</li>
<li><strong>Version Control</strong>: Track evaluation results alongside model versions</li>
<li><strong>Reproducibility</strong>: Document exact evaluation parameters</li>
<li><strong>Reality Check</strong>: Supplement metrics with visual inspection</li>
</ol>
<h2 id="troubleshooting-evaluation">Troubleshooting Evaluation</h2>
<p>Common issues and solutions:</p>
<ul>
<li><strong>Inconsistent Results</strong>: Increase number of evaluation episodes</li>
<li><strong>Model Loading Errors</strong>: Verify model type matches what's specified</li>
<li><strong>Environment Errors</strong>: Check that specified game states exist in your ROM</li>
<li><strong>Performance Gaps</strong>: Compare against baseline models or human performance</li>
<li><strong>Resource Usage</strong>: For large evaluations, reduce render quality or use no rendering</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="curriculum_learning.html" class="btn btn-neutral float-left" title="Curriculum Learning"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="model_finetuning.html" class="btn btn-neutral float-right" title="Fine-tuning Models">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="curriculum_learning.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="model_finetuning.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
