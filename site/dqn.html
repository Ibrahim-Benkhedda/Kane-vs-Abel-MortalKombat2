<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>DQN - Kane vs Abel Arena</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "DQN";
        var mkdocs_page_input_path = "dqn.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Kane vs Abel Arena
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="arena.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Environment</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="action_space.html">Action Space</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="adding_diff_game.html">Additional Games</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Agents</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="behaviour_tree.html">Behaviour Tree</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="human_input.html">Human Input</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">DQN</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#basic-dqn-algorithm">Basic DQN Algorithm</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#core-components">Core Components:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#training-loop">Training Loop:</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#implemented-variants">Implemented Variants</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-double-dqn">1. Double DQN</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-dueling-dqn">2. Dueling DQN</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3-dueling-double-dqn">3. Dueling Double DQN</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#comparing-performance">Comparing Performance</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#using-different-dqn-variants-in-the-arena">Using Different DQN Variants in the Arena</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#implementation-references">Implementation References</a>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RL Training</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="model_training.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="curriculum_learning.html">Curriculum Learning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Model Evaluation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="model_finetuning.html">Fine-tuning Models</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Kane vs Abel Arena</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Agents</li>
      <li class="breadcrumb-item active">DQN</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="deep-q-learning-in-kane-vs-abel-mortal-kombat-ii">Deep Q-Learning in Kane vs Abel Mortal Kombat II</h1>
<p>This document explains the different Deep Q-Learning variants implemented in the project, their key differences, and how they work.</p>
<h2 id="basic-dqn-algorithm">Basic DQN Algorithm</h2>
<h3 id="core-components">Core Components:</h3>
<ul>
<li><strong>Q-Network</strong>: Neural network that approximates the Q-value function</li>
<li><strong>Replay Buffer</strong>: Stores experience tuples (state, action, reward, next_state, done)</li>
<li><strong>Target Network</strong>: Copy of Q-network for stable target calculation</li>
<li><strong>Exploration Policy</strong>: Typically epsilon-greedy for balancing exploration and exploitation</li>
</ul>
<h3 id="training-loop">Training Loop:</h3>
<ol>
<li><strong>Initialize Environment and Replay Buffer</strong>:</li>
<li>
<p>Start with an empty replay buffer to store experiences</p>
</li>
<li>
<p><strong>Initialize Q-Networks</strong>:</p>
</li>
<li>Policy network (for action selection)</li>
<li>
<p>Target network (periodically updated copy)</p>
</li>
<li>
<p><strong>For each episode</strong>:</p>
</li>
<li>Reset environment to initial state</li>
<li>For each step until episode ends:<ul>
<li>Select action using epsilon-greedy policy</li>
<li>Execute action, observe reward and next state</li>
<li>Store experience in replay buffer</li>
<li>Sample mini-batch from replay buffer</li>
<li>Compute target Q-values: $y = r + \gamma \max_{a'} Q_{target}(s', a')$</li>
<li>Update Q-network by minimizing loss: $L = \frac{1}{N} \sum (y - Q(s,a))^2$</li>
<li>Periodically update target network</li>
</ul>
</li>
</ol>
<h2 id="implemented-variants">Implemented Variants</h2>
<h3 id="1-double-dqn">1. Double DQN</h3>
<p><strong>Implementation</strong>: <code>mk_ai.agents.DQN.double_dqn.py</code></p>
<p><strong>Key Improvement</strong>: Addresses overestimation bias in standard DQN</p>
<p><strong>How It Works</strong>:
- Uses the online network to select actions
- Uses the target network to evaluate those actions
- Target calculation: $y = r + \gamma Q_{target}(s', \arg\max_{a'} Q_{online}(s', a'))$</p>
<p><strong>Code Highlight</strong>:</p>
<pre><code class="language-python"># Select action with online network
next_actions_online = th.argmax(next_q_values_online, dim=1)
# Use target network to evaluate action
next_q_values = th.gather(next_q_values, dim=1, 
                          index=next_actions_online.unsqueeze(1)).flatten()
</code></pre>
<h3 id="2-dueling-dqn">2. Dueling DQN</h3>
<p><strong>Implementation</strong>: dueling_dqn.py</p>
<p><strong>Key Improvement</strong>: Separates state value and action advantage estimation</p>
<p><strong>How It Works</strong>:
- Splits the Q-network into two streams:
  - Value stream: estimates state value V(s)
  - Advantage stream: estimates action advantages A(s,a)
- Combines them: $Q(s,a) = V(s) + (A(s,a) - \frac{1}{|A|}\sum_a A(s,a))$</p>
<p><strong>Network Architecture</strong>:
- Shared feature extractor (usually CNN for game images)
- Separate value and advantage heads
- Special aggregation layer to combine outputs</p>
<h3 id="3-dueling-double-dqn">3. Dueling Double DQN</h3>
<p><strong>Implementation</strong>: dueling_ddqn.py</p>
<p><strong>Key Improvement</strong>: Combines both Double DQN and Dueling architecture benefits</p>
<p><strong>How It Works</strong>:
- Uses Dueling architecture (separate value and advantage streams)
- Applies Double DQN update rule (decoupling action selection from evaluation)
- Inherits from Double DQN but uses Dueling network architecture</p>
<p><strong>Network Structure</strong>:</p>
<pre><code>┌──────────────────┐
│  Feature Layers  │
└────────┬─────────┘
         │
         ▼
┌─────────────────┐     ┌──────────────────┐
│   Value Stream  │     │ Advantage Stream │
└────────┬────────┘     └────────┬─────────┘
         │                       │
         ▼                       ▼
┌────────────────────────────────────────┐
│      Q(s,a) = V(s) + A(s,a) - mean     │
└────────────────────────────────────────┘
</code></pre>
<h2 id="comparing-performance">Comparing Performance</h2>
<p>In Mortal Kombat II gameplay, we've observed:</p>
<ul>
<li><strong>Standard DQN</strong>: Tends to overestimate action values, leading to suboptimal policies</li>
<li><strong>Double DQN</strong>: More conservative value estimates, steadier learning</li>
<li><strong>Dueling DQN</strong>: Better at estimating state values independent of actions</li>
<li><strong>Dueling Double DQN</strong>: Best overall performance, combining the strengths of both approaches</li>
</ul>
<h2 id="using-different-dqn-variants-in-the-arena">Using Different DQN Variants in the Arena</h2>
<p>The Arena system allows selecting different DQN variants:</p>
<pre><code class="language-bash"># Standard DQN
python arena.py --p1-type dqn --p1-model models/dqn_model

# Double DQN
python arena.py --p1-type double_dqn --p1-model models/double_dqn_model

# Dueling DQN (architecturally different but same update rule as DQN)
python arena.py --p1-type dqn --p1-model models/dueling_dqn_model

# Dueling Double DQN (both architectural and update rule changes)
python arena.py --p1-type dueling_ddqn --p1-model models/dueling_double_dqn_model
</code></pre>
<h2 id="implementation-references">Implementation References</h2>
<ul>
<li>Double DQN: <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-Learning</a></li>
<li>Dueling DQN: <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="human_input.html" class="btn btn-neutral float-left" title="Human Input"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="model_training.html" class="btn btn-neutral float-right" title="Overview">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="human_input.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="model_training.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
