<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Fine-tuning Models - Kane vs Abel Arena</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Fine-tuning Models";
        var mkdocs_page_input_path = "model_finetuning.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Kane vs Abel Arena
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="arena.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Environment</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="action_space.html">Action Space</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="adding_diff_game.html">Additional Games</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Agents</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="behaviour_tree.html">Behaviour Tree</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="human_input.html">Human Input</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="dqn.html">DQN</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RL Training</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="model_training.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="curriculum_learning.html">Curriculum Learning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Model Evaluation</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Fine-tuning Models</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#what-is-fine-tuning">What is Fine-tuning?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#when-to-fine-tune">When to Fine-tune</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#using-the-fine-tuning-script">Using the Fine-tuning Script</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#basic-usage">Basic Usage</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#fine-tuning-process">Fine-tuning Process</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-select-a-pre-trained-model">1. Select a Pre-trained Model</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-define-challenging-states">2. Define Challenging States</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3-adjust-learning-rate-for-fine-tuning">3. Adjust Learning Rate for Fine-tuning</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#4-optionally-freeze-early-layers">4. Optionally Freeze Early Layers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#5-configure-evaluation-during-fine-tuning">5. Configure Evaluation During Fine-tuning</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#6-continue-training">6. Continue Training</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#customizing-fine-tuning">Customizing Fine-tuning</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#target-specific-weaknesses">Target Specific Weaknesses</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#learning-rate-schedule-strategies">Learning Rate Schedule Strategies</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#selective-layer-freezing">Selective Layer Freezing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#exploration-settings">Exploration Settings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#advanced-fine-tuning-techniques">Advanced Fine-tuning Techniques</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#regularization-for-fine-tuning">Regularization for Fine-tuning</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#elastic-weight-consolidation">Elastic Weight Consolidation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rehearsal-with-mixed-experiences">Rehearsal with Mixed Experiences</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#progressive-fine-tuning">Progressive Fine-tuning</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#evaluating-fine-tuning-success">Evaluating Fine-tuning Success</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#example-complete-fine-tuning-workflow">Example: Complete Fine-tuning Workflow</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#best-practices-for-fine-tuning">Best Practices for Fine-tuning</a>
    </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Kane vs Abel Arena</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">RL Training</li>
      <li class="breadcrumb-item active">Fine-tuning Models</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="fine-tuning-rl-models-for-mortal-kombat-ii">Fine-tuning RL Models for Mortal Kombat II</h1>
<p>This guide explains how to fine-tune pre-trained reinforcement learning models to improve their performance in specific scenarios or against challenging opponents.</p>
<h2 id="what-is-fine-tuning">What is Fine-tuning?</h2>
<p>Fine-tuning is the process of continuing training on a pre-trained model, focusing on specific areas that need improvement. It's a form of transfer learning that leverages knowledge gained from prior training to efficiently adapt to new challenges.</p>
<h2 id="when-to-fine-tune">When to Fine-tune</h2>
<p>Consider fine-tuning your models when:</p>
<ul>
<li>They perform well overall but struggle against specific opponents</li>
<li>You want to improve performance in advanced scenarios without starting from scratch</li>
<li>You need to adapt a general model to a specialized task</li>
<li>You've reached a performance plateau with regular training</li>
<li>You have limited computational resources for full retraining</li>
</ul>
<h2 id="using-the-fine-tuning-script">Using the Fine-tuning Script</h2>
<p>The Kane vs Abel framework includes a dedicated fine-tuning script (<code>finetune.py</code>) that handles loading pre-trained models and continuing their training on challenging scenarios.</p>
<h3 id="basic-usage">Basic Usage</h3>
<pre><code class="language-bash">python finetune.py
</code></pre>
<p>By default, this will:
1. Load the model specified in the script
2. Create environments with challenging states
3. Continue training for a defined number of timesteps
4. Save the fine-tuned model</p>
<h2 id="fine-tuning-process">Fine-tuning Process</h2>
<h3 id="1-select-a-pre-trained-model">1. Select a Pre-trained Model</h3>
<p>Choose a well-performing base model to fine-tune:</p>
<pre><code class="language-python">pretrained_model_path = os.path.join(MODEL_DIR, &quot;kane&quot;, &quot;DuellingDDQN_curriculum_16M_VeryEasy_3_Tiers&quot;)
model = DuelingDoubleDQN.load(pretrained_model_path, env=env, device=&quot;cuda&quot;)
</code></pre>
<h3 id="2-define-challenging-states">2. Define Challenging States</h3>
<p>Select specific challenging states where the model needs improvement:</p>
<pre><code class="language-python">challenging_states = [
    &quot;VeryEasy.LiuKang-04&quot;,
    &quot;VeryEasy.LiuKang-05&quot;,
    &quot;VeryEasy.LiuKang-06&quot;,
    &quot;VeryEasy.LiuKang-07&quot;,
    &quot;VeryEasy.LiuKang-08&quot;
]
</code></pre>
<h3 id="3-adjust-learning-rate-for-fine-tuning">3. Adjust Learning Rate for Fine-tuning</h3>
<p>Use a more conservative learning rate to avoid catastrophic forgetting:</p>
<pre><code class="language-python"># Lower starting rate and gentler decay for fine-tuning
fine_tune_lr = Schedules.linear_decay(1e-4, 1e-5)  # Original might be 3e-4 to 1e-5
model.learning_rate = fine_tune_lr
</code></pre>
<h3 id="4-optionally-freeze-early-layers">4. Optionally Freeze Early Layers</h3>
<p>To preserve learned features while allowing adaptation in higher layers:</p>
<pre><code class="language-python"># Uncomment to freeze feature extractor
# for param in model.policy.features_extractor.parameters():
#     param.requires_grad = False
</code></pre>
<h3 id="5-configure-evaluation-during-fine-tuning">5. Configure Evaluation During Fine-tuning</h3>
<p>Set up evaluation callbacks to monitor progress:</p>
<pre><code class="language-python">eval_callback = CustomEvalCallback(
    eval_env=eval_env,
    best_model_save_path=os.path.join(LOG_DIR, &quot;fine_tuned_best&quot;),
    log_path=os.path.join(LOG_DIR, &quot;fine_tune_eval&quot;),
    eval_freq=62_500,  # How often to evaluate
    n_eval_episodes=10,
    deterministic=False,
    render=False,
    verbose=1
)
</code></pre>
<h3 id="6-continue-training">6. Continue Training</h3>
<p>Train for additional timesteps, typically fewer than the original training:</p>
<pre><code class="language-python">model.learn(
    total_timesteps=2_000_000,  # Less than original training
    reset_num_timesteps=False,  # Continue from previous steps
    callback=eval_callback,
)
</code></pre>
<h2 id="customizing-fine-tuning">Customizing Fine-tuning</h2>
<h3 id="target-specific-weaknesses">Target Specific Weaknesses</h3>
<p>Identify and focus on specific weaknesses through careful state selection:</p>
<pre><code class="language-python"># Example: Focus on specific opponents
challenging_states = [
    &quot;Medium.LiuKangVsBaraka&quot;,  # If struggling against Baraka
    &quot;Hard.LiuKangVsReptile&quot;     # If struggling against Reptile
]
</code></pre>
<h3 id="learning-rate-schedule-strategies">Learning Rate Schedule Strategies</h3>
<p>Different schedules for different fine-tuning goals:</p>
<pre><code class="language-python"># For minor adjustments (conservative)
conservative_lr = Schedules.linear_decay(5e-5, 1e-6)

# For significant adaptation (more aggressive)
adaptive_lr = Schedules.linear_decay(1e-4, 1e-5)

# For focused, short fine-tuning
cyclical_lr = Schedules.cyclical_lr(5e-5, 1e-4, 0.5)
</code></pre>
<h3 id="selective-layer-freezing">Selective Layer Freezing</h3>
<p>For more control over what parts of the model adapt:</p>
<pre><code class="language-python"># Option 1: Freeze just the convolutional base
for name, param in model.policy.features_extractor.cnn.named_parameters():
    param.requires_grad = False

# Option 2: Freeze specific layers
for name, param in model.policy.named_parameters():
    if &quot;features_extractor&quot; in name:
        param.requires_grad = False
    if &quot;q_net.0&quot; in name:  # First layer after feature extraction
        param.requires_grad = False
</code></pre>
<h3 id="exploration-settings">Exploration Settings</h3>
<p>Adjust exploration parameters for fine-tuning:</p>
<pre><code class="language-python"># Reduce exploration for fine-tuning
model.exploration_schedule = Schedules.linear_decay(0.1, 0.01)  # Lower than original
</code></pre>
<h2 id="advanced-fine-tuning-techniques">Advanced Fine-tuning Techniques</h2>
<h3 id="regularization-for-fine-tuning">Regularization for Fine-tuning</h3>
<p>Add regularization to prevent overfitting to the new states:</p>
<pre><code class="language-python"># Add L2 regularization to optimizer (example for Adam)
from torch import optim

# Get current params
optimizer_params = model.policy.optimizer.defaults
# Add weight decay (L2 regularization)
optimizer_params['weight_decay'] = 1e-4
# Recreate optimizer
model.policy.optimizer = optim.Adam(
    model.policy.parameters(), 
    **optimizer_params
)
</code></pre>
<h3 id="elastic-weight-consolidation">Elastic Weight Consolidation</h3>
<p>For complex models, implement Elastic Weight Consolidation (EWC) to preserve important weights:</p>
<pre><code class="language-python"># Pseudo-code for EWC implementation
original_params = {name: param.clone().detach() for name, param in model.named_parameters()}
fisher_information = estimate_fisher_information(model, old_env)

def ewc_loss(model, original_params, fisher_information, lambda_ewc=5000):
    loss = 0
    for name, param in model.named_parameters():
        loss += fisher_information[name] * (param - original_params[name]).pow(2).sum()
    return lambda_ewc * loss
</code></pre>
<h3 id="rehearsal-with-mixed-experiences">Rehearsal with Mixed Experiences</h3>
<p>Fine-tune with a mix of old and new experiences to prevent forgetting:</p>
<pre><code class="language-python"># Create environments with mixed states
mixed_states = original_training_states + challenging_states
mixed_env = SubprocVecEnv([make_env(mixed_states) for _ in range(NUM_ENVS)])
</code></pre>
<h3 id="progressive-fine-tuning">Progressive Fine-tuning</h3>
<p>Gradually introduce challenging scenarios:</p>
<pre><code class="language-python"># Start with a mix favoring original states
phase1_states = original_states + [challenging_states[0]]
# Then introduce more challenging states
phase2_states = original_states + challenging_states[:3]
# Finally use all challenging states
phase3_states = challenging_states
</code></pre>
<h2 id="evaluating-fine-tuning-success">Evaluating Fine-tuning Success</h2>
<p>After fine-tuning, evaluate comprehensively:</p>
<pre><code class="language-bash"># Evaluate on original states
python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states &quot;original_state1,original_state2&quot; --individual_eval

python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states &quot;original_state1,original_state2&quot; --individual_eval

# Evaluate on challenging states
python test.py --model_path models/pre_finetuned.zip --model_type DUELINGDDQN --states &quot;challenging_state1,challenging_state2&quot; --individual_eval

python test.py --model_path models/post_finetuned.zip --model_type DUELINGDDQN --states &quot;challenging_state1,challenging_state2&quot; --individual_eval
</code></pre>
<p>Look for:
1. Improvement on challenging states
2. Maintenance of performance on original states
3. Overall win rate changes
4. Changes in behavior patterns</p>
<h2 id="example-complete-fine-tuning-workflow">Example: Complete Fine-tuning Workflow</h2>
<pre><code class="language-python">import os
from mk_ai.agents import DuelingDoubleDQN
from mk_ai.utils import Schedules
from mk_ai.callbacks import CustomEvalCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack

# 1. Identify where the model is struggling (through evaluation)
# python test.py --model_path models/base_model.zip --model_type DUELINGDDQN --states &quot;all_states&quot; --individual_eval

# 2. Select challenging states based on evaluation
challenging_states = [
    &quot;VeryEasy.LiuKang-06&quot;,  # Low win rate identified here
    &quot;VeryEasy.LiuKang-07&quot;,  # Low win rate identified here
    &quot;VeryEasy.LiuKang-08&quot;   # Low win rate identified here
]

# 3. Create environments for fine-tuning
env = SubprocVecEnv([make_env(challenging_states) for _ in range(8)])
env = VecFrameStack(env, n_stack=4)

# 4. Load pre-trained model
model = DuelingDoubleDQN.load(&quot;models/base_model.zip&quot;, env=env, device=&quot;cuda&quot;)

# 5. Configure for fine-tuning
model.learning_rate = Schedules.linear_decay(1e-4, 1e-5)

# 6. Setup evaluation
eval_env = DummyVecEnv([make_env(challenging_states)])
eval_env = VecFrameStack(eval_env, n_stack=4)
eval_callback = CustomEvalCallback(
    eval_env=eval_env,
    best_model_save_path=&quot;./logs/fine_tuned_best&quot;,
    log_path=&quot;./logs/fine_tune_eval&quot;,
    eval_freq=60000,
    n_eval_episodes=10
)

# 7. Fine-tune the model
model.learn(
    total_timesteps=2_000_000,
    reset_num_timesteps=False,
    callback=eval_callback
)

# 8. Save fine-tuned model
model.save(&quot;models/fine_tuned_model&quot;)

# 9. Evaluate the fine-tuned model
# python test.py --model_path models/fine_tuned_model.zip --model_type DUELINGDDQN --states &quot;all_states&quot; --individual_eval
</code></pre>
<h2 id="best-practices-for-fine-tuning">Best Practices for Fine-tuning</h2>
<ol>
<li><strong>Always evaluate before and after</strong> to quantify improvements</li>
<li><strong>Start with a lower learning rate</strong> than the original training</li>
<li><strong>Be selective with states</strong> - target specific weaknesses</li>
<li><strong>Monitor for catastrophic forgetting</strong> on original scenarios</li>
<li><strong>Keep fine-tuning sessions shorter</strong> than original training</li>
<li><strong>Save intermediate checkpoints</strong> during fine-tuning</li>
<li><strong>Consider layer freezing</strong> for specialized adaptations</li>
<li><strong>Use TensorBoard</strong> to monitor the fine-tuning process</li>
</ol>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="model_evaluation.html" class="btn btn-neutral float-left" title="Model Evaluation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="model_evaluation.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
